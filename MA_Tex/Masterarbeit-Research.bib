Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Doring2015,
abstract = {Der Klassiker zu den Forschungsmethoden – rundum erneuert, didaktisch verbessert und aktueller denn je! Dieses Buch ist ein fundierter und verl{\"{a}}sslicher Begleiter f{\"{u}}r Studierende, Forschende und Berufst{\"{a}}tige – da ist alles drin:Grundlagen: Wissenschaftstheorie, Qualit{\"{a}}tskriterien sowie ethische Aspekte.Anwendung: Alle Phasen des Forschungsprozesses von der Festlegung des Forschungsthemas, des Untersuchungsdesigns und der Operationalisierung {\"{u}}ber Stichprobenziehung, Datenerhebungs- und Datenanalysemethoden bis zur Ergebnispr{\"{a}}sentation.Vertiefung: Effektgr{\"{o}}{\ss}en, Metaanalysen, Strukturgleichungsmodelle, Evaluationsforschung.Die 5. Auflage wurde grundlegend {\"{u}}berarbeitet:Klarheit: Verbesserte Gliederung der Kapitel sowie des gesamten Buches.Aktualit{\"{a}}t: Beitr{\"{a}}ge zu Online-Methoden, Mixed-Methods-Designs und anderen neueren Entwicklungen.Lernfreundlichkeit: Viele Abbildungen, Tabellen, Definitionsboxen, Cartoons, {\"{U}}bungsaufgaben und Lernquiz mit L{\"{o}}sungen.Praxisbezug: Reale Studienbeispiele aus verschiedenen sozial- und humanwissenschaftlichen F{\"{a}}chern (z.B. Psychologie, Kommunikationswissenschaft, Erziehungswissenschaft, Medizin, Soziologie).Eine Begleit-Website bietet Lern-Tools f{\"{u}}r Studierende und Materialien f{\"{u}}r Lehrende.},
author = {D{\"{o}}ring, Nicola and Bortz, J{\"{u}}rgen},
doi = {10.1007/978-3-642-41089-5},
isbn = {3642410898},
pages = {1051},
title = {{Forschungsmethoden und Evaluation in den Sozial- und Humanwissenschaften}},
url = {https://books.google.com/books?id=EvfNCgAAQBAJ{\&}pgis=1},
year = {2015}
}
@article{Hantke2016,
abstract = {Crowdsourcing is an arising collaborative approach applicable among many other applications to the area of language and speech processing. In fact, the use of crowdsourcing was already applied in the field of speech processing with promising results. However, only few studies investigated the use of crowdsourcing in computational paralinguistics. In this contribution, we propose a novel evaluator for crowdsourced-based ratings termed Weighted Trustability Evaluator (WTE) which is computed from the rater-dependent consistency over the test questions. We further investigate the reliability of crowdsourced annotations as compared to the ones obtained with traditional labelling procedures, such as constrained listening experiments in laboratories or in controlled environments. This comparison includes an in-depth analysis of obtainable classification performances. The experiments were conducted on the Speaker Likability Database (SLD) already used in the INTERSPEECH Challenge 2012, and the results lend further weight to the assumption that crowdsourcing can be applied as a reliable annotation source for computational paralinguistics given a sufficient number of raters and suited measurements of their reliability.},
author = {Hantke, Simone and Marchi, Erik and Schuller, Bj{\"{o}}rn},
file = {:home/mhemmer/Downloads/MA-related/Literatur/EWE/Hantke16-ITW.pdf:pdf},
isbn = {9782951740891},
journal = {Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016},
keywords = {Computational paralinguistics,Crowdsourcing,Speech classification,Speech corpus annotation},
number = {2009},
pages = {2156--2161},
title = {{Introducing the weighted trustability evaluator for crowdsourcing exemplified by speaker likability classification}},
year = {2016}
}
@article{Grimm2005,
abstract = {Emotion recognition in speech is an important research objective in the field of man-machine interfaces. This paper focuses on human labeling of emotions to create training data for emotion recognition systems. A three-dimensional emotion space concept is used to address the complexity of emotions in natural speech. As an evaluation tool, an iconic representation of each emotion component (Self Assessment Manikins, SAMs) is proposed. This method is shown to be a simple and efficient means for evaluating emotions at an utterance-based segmentation level. The results show a high inter-evaluator agreement and a good reliability with the help of statistical signal modeling. {\textcopyright} 2005 IEEE.},
author = {Grimm, Michael and Kroschel, Kristian},
doi = {10.1109/ASRU.2005.1566530},
file = {:home/mhemmer/Downloads/MA-related/Literatur/EWE/Grimm05-EON.pdf:pdf},
isbn = {0780394798},
journal = {Proceedings of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop},
pages = {381--385},
title = {{Evaluation of natural emotions using self assessment manikins}},
volume = {2005},
year = {2005}
}
