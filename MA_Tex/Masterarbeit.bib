Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@phdthesis{Vaux1994,
abstract = {This thesis develops an analysis of the phonological system of modern standard Armenian within Rules and Representations Theory, a model of phonology which assumes full specification in underlying representations and rule-driven derivations. I focus on the impact of Armenian data on current theories of underspecification, feature geometry, consonant-vowel interactions, syllabification, vowel harmony, and constraints and repairs. Some of the more interesting theoretical claims of this thesis are that languages can assign consonants to syllable nuclei at an intermediate point in derivations, even though these consonants always surface as syllable margins; uvulars, epiglottals, and pharyngeals are in a relationship analogous to that between high, mid, and low vowels, and the Lower Vocal Tract node dominating these segments occupies a position directly parallel to the traditional Place node; contour segments show only edge effects and no anti-edge effects; voiceless but not voiced fricatives are (+spread glottis); the common phenomenon of nasal voicing is controlled by a marking statement * (+nasal, +spread glottis), contrary to the traditional view involving the configuration * (+nasal, {\$}-{\$}continuant); and voiced aspirates are best viewed as simple segments characterized by the features ({\$}-{\$}stiff vocal folds, +spread glottis). The thesis is divided into five chapters: the first sets out the theoretical assumptions on which I base my work; chapter two presents a basic overview of Armenian phonology focusing on stress assignment, syllabification, and vowel harmony; chapter three develops an account of syllabification and epenthesis in the literary dialects; chapter four considers evidence bearing on the feature geometric component of universal grammar; and chapter five examines the phonological behavior of laryngeal features in the Armenian dialect of New Julfa and presents an analysis of the consonant shifts which occurred in the modern Armenian and Indo-Aryan dialects.},
author = {Vaux, Bert Richard},
booktitle = {ProQuest Dissertations and Theses},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaux - Unknown - Armenian Phonology.pdf:pdf},
keywords = {0290:Linguistics,Altaic,Indo-Aryan,Language,Linguistics,dialects,literature and linguistics,syllabification},
pages = {343},
title = {{Armenian phonology}},
url = {https://www.academia.edu/300596/Armenian{\_}Phonology http://login.proxy.library.vanderbilt.edu/login?url=http://search.proquest.com/docview/304112951?accountid=14816{\%}5Cnhttp://sfx.library.vanderbilt.edu/vu?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mt},
year = {1994}
}
@techreport{Scherer2001,
abstract = {Whereas the perception of emotion from facial expression has been extensively studied cross-culturally, little is known about judges' ability to infer emotion from vocal cues. This article reports the results from a study conducted in nine countries in Europe, the United States, and Asia on vocal emotion portrayals of anger, sadness, fear, joy, and neutral voice as produced by professional German actors. Data show an overall accuracy of 66{\%} across all emotions and countries. Although accuracy was substantially better than chance, there were sizable differences ranging from 74{\%} in Germany to 52{\%} in Indonesia. However, patterns of confusion were very similar across all countries. These data suggest the existence of similar inference rules from vocal expression across cultures. Generally, accuracy decreased with increasing language dissimilarity from German in spite of the use of language-free speech samples. It is concluded that culture-and language specific paralinguistic patterns may influence the decoding process. One of the key issues of current debate in the psychology of emotion concerns the universal-ity versus cultural relativity of emotional expression. This has important implications for the central question of the nature and function of emotion. Although there is a general consensus that both biological and cultural factors contribute to the emotion process (see Mesquita, Frijda, {\&} Scherer, 1997), the relative contribution of each of the factors, or the respective amount of variance explained, remains to be explored. An ideal way to study this issue empirically is to compare outward manifestations of emotional reactions with similar appraisals of eliciting situations in different cultures (see Scherer, 1997). Such studies could reveal the extent to which similar expression configurations indicate comparable evaluation and reaction tendencies. Unfortunately, given the difficulty of identifying and systematically studying comparable eliciting situations in different cultures, such studies have yet to be conducted. Instead, researchers in this area have adopted an indirect approach in addressing the issue. This approach is based on the assumption that, given the indisputable role of emotional expression in social communication, the ability of members of one culture to correctly 76 AUTHORS' NOTE: This research has been conducted as a collaborative research program in the context of the Coordination Europ{\'{e}}enne de la Recherche sur les Emotions (CERE), which is formed by the laboratories directed by Matty Chiva,},
author = {Scherer, Klaus R and Banse, Rainer and Wallbott, Harald G and Rim{\'{e}}, Bernard and Kappas, A and Manstead, A and Ricci-Bitti, P},
booktitle = {VOCAL EMOTION EXPRESSION JOURNAL OF CROSS-CULTURAL PSYCHOLOGY},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scherer et al. - 2001 - EMOTION INFERENCES FROM VOCAL EXPRESSION CORRELATE ACROSS LANGUAGES AND CULTURES.pdf:pdf},
number = {1},
pages = {76--92},
title = {{EMOTION INFERENCES FROM VOCAL EXPRESSION CORRELATE ACROSS LANGUAGES AND CULTURES}},
volume = {32},
year = {2001}
}
@article{Schirmer2019,
abstract = {This study examined how trustworthiness impressions depend on vocal expressive and person characteristics and how their dependence may be explained by acoustical profiles. Sentences spoken in a range of emotional and conversational expressions by 20 speakers differing in age and sex were presented to 80 age and sex matched listeners who rated speaker trustworthiness. Positive speaker valence but not arousal consistently predicted greater perceived trustworthiness. Additionally, voices from younger as compared with older and female as compared with male speakers were judged more trustworthy. Acoustic analysis highlighted several parameters as relevant for differentiating trustworthiness ratings and showed that effects largely overlapped with those for speaker valence and age, but not sex. Specifically, a fast speech rate, a low harmonic-to-noise ratio, and a low fundamental frequency mean and standard deviation differentiated trustworthy from untrustworthy, positive from negative, and younger from older voices. Male and female voices differed in other ways. Together, these results show that a speaker's expressive as well as person characteristics shape trustworthiness impressions and that their effect likely results from a combination of low-level perceptual and higher-order conceptual processes.},
author = {Schirmer, Annett and Feng, Yenju and Sen, Antarika and Penney, Trevor B.},
doi = {10.1371/journal.pone.0210555},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirmer et al. - 2019 - Angry, old, male – and trustworthy How expressive and person voice characteristics shape listener trust.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {jan},
number = {1},
publisher = {Public Library of Science},
title = {{Angry, old, male – and trustworthy? How expressive and person voice characteristics shape listener trust}},
volume = {14},
year = {2019}
}
@book{Dum-Tragut,
author = {Dum-Tragut, Jasmine},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dum-Tragut - Unknown - Armenian. Modern Eastern Armenian(2).pdf:pdf;:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dum-Tragut - Unknown - Armenian. Modern Eastern Armenian.pdf:pdf},
title = {{Armenian. Modern Eastern Armenian}}
}
@article{Sander2018,
abstract = {Modeling emotion processes remains a conceptual and methodological challenge in affective sciences. In responding to the other target articles in this special section on “Emotion and the Brain” and the comments on our article, we address the issue of potentially separate brain networks subserving the functions of the different emotion components. In particular, we discuss the suggested role of component synchronization in producing information integration for the dynamic emergence of a coherent emotion process, as well as the links between incentive salience (“wanting”) and concern-relevance in the elicitation of emotion.},
author = {Sander, David and Grandjean, Didier and Scherer, Klaus R.},
doi = {10.1177/1754073918783257},
issn = {17540739},
journal = {Emotion Review},
keywords = {emotion components,emotional brain,relevance,synchronization},
month = {jul},
number = {3},
pages = {238--241},
publisher = {SAGE Publications Ltd},
title = {{Brain Networks, Emotion Components, and Appraised Relevance}},
volume = {10},
year = {2018}
}
@misc{Schirmer2006,
abstract = {Vocal perception is particularly important for understanding a speaker's emotional state and intentions because, unlike facial perception, it is relatively independent of speaker distance and viewing conditions. The idea, derived from brain lesion studies, that vocal emotional comprehension is a special domain of the right hemisphere has failed to receive consistent support from neuroimaging. This conflict can be reconciled if vocal emotional comprehension is viewed as a multi-step process with individual neural representations. This view reveals a processing chain that proceeds from the ventral auditory pathway to brain structures implicated in cognition and emotion. Thus, vocal emotional comprehension appears to be mediated by bilateral mechanisms anchored within sensory, cognitive and emotional processing systems. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Schirmer, Annett and Kotz, Sonja A.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/j.tics.2005.11.009},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Beyond{\_}the{\_}right{\_}hemisphere{\_}brain{\_}mechan.pdf:pdf},
issn = {13646613},
month = {jan},
number = {1},
pages = {24--30},
title = {{Beyond the right hemisphere: Brain mechanisms mediating vocal emotional processing}},
volume = {10},
year = {2006}
}
@article{Pell2020,
abstract = {To investigate the impact of culture on emotion processing, we conducted a study comparing the intensity ratings of external expression (intensity level of speaker's vocal expression) and speaker's internal feeling (intensity level participants think the speaker is experiencing). Specifically, a group of Canadian and Chinese participants categorized emotions (anger, fear, happiness, and sadness) and judged the intensity of emotional utterances in three languages (Chinese, English, and Hindi). Both groups were more accurate at recognizing emotions in their native language. In contrast to related work on facial expressions, which concluded that Eastern participants were more likely to assume that speakers experienced more intense feelings than what they expressed, compared to Western participants, our study on vocal expressions did not find similar cultural effects. Both Canadian and Chinese participants rated the internal feelings of speakers as more intense than their external expressions of emotion. Differences between studies are discussed in terms of the unique structure and social functions of vocal and facial expressions in communication and their interactions with culture.},
author = {Pell, Marc D and Zhang, Shuyi and Pell, Marc D},
doi = {10.13140/RG.2.2.36159.46246},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pell, Zhang, Pell - 2020 - Cross-cultural Differences in Vocal Expression and Emotion Perception Cross-cultural Differences in Vocal Exp.pdf:pdf},
number = {March},
title = {{Cross-cultural Differences in Vocal Expression and Emotion Perception Cross-cultural Differences in Vocal Expression and Emotion Perception}},
url = {https://www.researchgate.net/publication/339988042{\_}Cross-cultural{\_}Differences{\_}in{\_}Vocal{\_}Expression{\_}and{\_}Emotion{\_}Perception},
year = {2020}
}
@article{Droit-Volet2007,
abstract = {Our sense of time is altered by our emotions to such an extent that time seems to fly when we are having fun and drags when we are bored. Recent studies using standardized emotional material provide a unique opportunity for understanding the neurocognitive mechanisms that underlie the effects of emotion on timing and time perception in the milliseconds-to-hours range. We outline how these new findings can be explained within the framework of internal-clock models and describe how emotional arousal and valence interact to produce both increases and decreases in attentional time sharing and clock speed. The study of time and emotion is at a crossroads, and we outline possible examples for future directions. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Droit-Volet, Sylvie and Meck, Warren H.},
doi = {10.1016/j.tics.2007.09.008},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Droit-Volet, Meck - 2007 - How emotions colour our perception of time.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {dec},
number = {12},
pages = {504--513},
pmid = {18023604},
title = {{How emotions colour our perception of time}},
volume = {11},
year = {2007}
}
@article{Ackermann2014c,
abstract = {In this response to commentaries, we revisit the two main arguments of our target article. Based on data drawn from a variety of research areas - vocal behavior in nonhuman primates, speech physiology and pathology, neurobiology of basal ganglia functions, motor skill learning, paleoanthropological concepts - the target article, first, suggests a two-stage model of the evolution of the crucial motor prerequisites of spoken language within the hominin lineage: (1) monosynaptic refinement of the projections of motor cortex to brainstem nuclei steering laryngeal muscles, and (2) subsequent vocal-laryngeal elaboration of cortico-basal ganglia circuits, driven by human-specific FOXP2 mutations. Second, as concerns the ontogenetic development of verbal communication, age-dependent interactions between the basal ganglia and their cortical targets are assumed to contribute to the time course of the acquisition of articulate speech. Whereas such a phylogenetic reorganization of cortico-striatal circuits must be considered a necessary prerequisite for ontogenetic speech acquisition, the 30 commentaries - addressing the whole range of data sources referred to - point at several further aspects of acoustic communication which have to be added to or integrated with the presented model. For example, the relationships between vocal tract movement sequencing - the focus of the target article - and rhythmical structures of movement organization, the connections between speech motor control and the central-auditory and central-visual systems, the impact of social factors upon the development of vocal behavior (in nonhuman primates and in our species), and the interactions of ontogenetic speech acquisition - based upon FOXP2-driven structural changes at the level of the basal ganglia - with preceding subvocal stages of acoustic communication as well as higher-order (cognitive) dimensions of phonological development. Most importantly, thus, several promising future research directions unfold from these contributions - accessible to clinical studies and functional imaging in our species as well as experimental investigations in nonhuman primates.},
author = {Ackermann, Hermann and Hage, Steffen R. and Ziegler, Wolfram},
doi = {10.1017/S0140525X1400003X},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ackermann, Hage, Ziegler - 2014 - Phylogenetic reorganization of the basal ganglia A necessary, but not the only, bridge over a primate.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
number = {6},
pages = {577--604},
title = {{Phylogenetic reorganization of the basal ganglia: A necessary, but not the only, bridge over a primate Rubicon of acoustic communication}},
url = {https://www.researchgate.net/publication/279296562{\_}Phylogenetic{\_}reorganization{\_}of{\_}the{\_}basal{\_}ganglia{\_}A{\_}necessary{\_}but{\_}not{\_}the{\_}only{\_}bridge{\_}over{\_}a{\_}primate{\_}Rubicon{\_}of{\_}acoustic{\_}communication},
volume = {37},
year = {2014}
}
@article{GlennSchellenberg2000,
abstract = {We examined effects of pitch and rhythm on the perceived emotional content of short melodies. We initially developed exemplars of melodies that were judged consistently to convey a single emotional category: happy, sad, or scary. We subsequently manipulated the pitch and rhythm parameters to derive three altered versions of each exemplar: a pitch-only version (pitch differences intact but all tones of equal duration), a rhythmonly version (durational differences intact but all tones of equal pitch), and a baseline version (all tones of equal duration and pitch). Listeners rated how well each exemplar and altered version conveyed its corresponding emotion. Effects of pitch and rhythm varied across melodies. In all cases, ratings were influenced more by differences in pitch than by differences in rhythm. Whenever rhythm affected ratings, it interacted with pitch. {\textcopyright} 2000 by the regents of the university of california all rights reserved.},
author = {{Glenn Schellenberg}, E. and Krysciak, Ania M. and {Jane Campbell}, R.},
doi = {10.2307/40285907},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schellenberg - 2000 - Perceiving Emotion in Melody Interactive Effects of Pitch and Rhythm Article in Music Perception.pdf:pdf},
issn = {07307829},
journal = {Music Perception},
number = {2},
pages = {155--171},
title = {{Perceiving emotion in melody: interactive effects of pitch and rhythm}},
url = {https://www.researchgate.net/publication/230746005},
volume = {18},
year = {2000}
}
@article{Baum1982,
abstract = {Three subtests of comprehension (one of varying sentential stress, one of shifting juncture, and one of applying emphatic stress to functors) were administered to eight Broca's aphasics and eight controls to determine the effects of stress and juncture on disambiguation of these sentences. Data indicated that the aphasic group performed significantly worse than the normals on all three subtests of comprehension. In addition, there was a strong positive correlation between severity of aphasia and error scores for all three tests. Results are discussed in relation to intact-vs.-disordered comprehension. {\textcopyright} 1982.},
author = {Baum, Shari R. and Daniloff, Joanne Kelsch and Daniloff, Raymond and Lewis, Jeffrey},
doi = {10.1016/0093-934X(82)90020-7},
issn = {10902155},
journal = {Brain and Language},
number = {2},
pages = {261--271},
title = {{Sentence comprehension by Broca's aphasics: Effects of some suprasegmental variables}},
volume = {17},
year = {1982}
}
@article{Jadoul2018a,
abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an efficient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Specifically, we focus on applications in data visualisation, file manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
author = {Jadoul, Yannick and Thompson, Bill and de Boer, Bart},
doi = {10.1016/j.wocn.2018.07.001},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Acoustics,Data analysis,Phonetics,Praat,Python,Software},
month = {nov},
pages = {1--15},
publisher = {Academic Press},
title = {{Introducing Parselmouth: A Python interface to Praat}},
volume = {71},
year = {2018}
}
@article{Parada-Cabaleiro2018a,
abstract = {The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion.},
author = {Parada-Cabaleiro, Emilia and Schmitt, Maximilian and Batliner, Anton and Hantke, Simone and Costantini, Giovanni and Scherer, Klaus and Schuller, Bj{\"{o}}rn W.},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro18-IEI.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
pages = {376--382},
title = {{Identifying emotions in opera singing: Implications of adverse acoustic conditions}},
year = {2018}
}
@article{Sander2005,
abstract = {Multiple levels of processing are thought to be involved in the appraisal of emotionally relevant events, with some processes being engaged relatively independently of attention, whereas other processes may depend on attention and current task goals or context. We conducted an event-related fMRI experiment to examine how processing angry voice prosody, an affectively and socially salient signal, is modulated by voluntary attention. To manipulate attention orthogonally to emotional prosody, we used a dichotic listening paradigm in which meaningless utterances, pronounced with either angry or neutral prosody, were presented simultaneously to both ears on each trial. In two successive blocks, participants selectively attended to either the left or right ear and performed a gender-decision on the voice heard on the target side. Our results revealed a functional dissociation between different brain areas. Whereas the right amygdala and bilateral superior temporal sulcus responded to anger prosody irrespective of whether it was heard from a to-be-attended or to-be-ignored voice, the orbitofrontal cortex and the cuneus in medial occipital cortex showed greater activation to the same emotional stimuli when the angry voice was to-be-attended rather than to-be-ignored. Furthermore, regression analyses revealed a strong correlation between orbitofrontal regions and sensitivity on a behavioral inhibition scale measuring proneness to anxiety reactions. Our results underscore the importance of emotion and attention interactions in social cognition by demonstrating that multiple levels of processing are involved in the appraisal of emotionally relevant cues in voices, and by showing a modulation of some emotional responses by both the current task-demands and individual differences. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Sander, David and Grandjean, Didier and Pourtois, Gilles and Schwartz, Sophie and Seghier, Mohamed L. and Scherer, Klaus R. and Vuilleumier, Patrik},
doi = {10.1016/j.neuroimage.2005.06.023},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Emotion{\_}and{\_}attention{\_}interactions{\_}in{\_}so.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Amygdala,Anger,Appraisal,Attention,Emotion,Orbitofrontal cortex,Prosody,STS},
number = {4},
pages = {848--858},
title = {{Emotion and attention interactions in social cognition: Brain regions involved in processing anger prosody}},
volume = {28},
year = {2005}
}
@article{Curry1967,
abstract = {Twenty-five left-handed and 25 right-handed subjects performed three dichotic listening tasks, two verbal and one non-verbal. Comparisons were made between mean scores obtained at the right and left ears, as well as between the handedness groups. The following results were obtained:1.The mean right ear score was higher than the mean left ear score for both handedness groups on both of the verbal dichotic tasks. This was significant for the right-handed group on both verbal tasks, but for the left-handed group it was significant on only one verbal task.2.The mean left ear score was higher than the mean right ear score for both handedness groups on the non-verbal dichotic task. This betweenears difference was statistically significant for the right-handed group only.3.Comparison of the two handedness groups on each of the three dichotic tests revealed that more left-handed subjects had ear leads which were the reverse of that found for the groups as a whole. This difference between the handedness groups, however, was statistically significant on only one of the dichotic tests.4.The mean left ear score was higher than the mean right ear score for both handedness groups on the non-verbal dichotic task. This betweenears difference was statistically significant for the right-handed group only.5.These data were re-analyzed comparing the size of the absolute between-ears difference scores of those individuals whose ear leads consistently were the reverse of the group as a whole, with subjects who showed no reversals. It was found that the reversal group had smaller mean scores on all three tests, and that this was significant on the two verbal tests. Although more left-handed subjects were found among the reversal group, the data tentatively indicate that the above finding holds true regardless of the handedness of the subjects in the reversal group. The above findings were interpreted as reflecting the different roles of the two cerebral hemispheres, as well as the degree of hemispheric equipotentiality.},
author = {Curry, Frederic K.W.},
doi = {10.1016/s0010-9452(67)80022-4},
issn = {00109452},
journal = {Cortex},
month = {sep},
number = {3},
pages = {343--352},
publisher = {Elsevier BV},
title = {{A Comparison of Left-Handed and Right-Handed Subjects on Verbal and Non-Verbal Dichotic Listening Tasks}},
volume = {3},
year = {1967}
}
@article{Boston2016,
author = {Boston, Amsterdam • and Heidelberg, • and London, • and York, New and Oxford, • and Paris, • and Diego, San and Mari{\"{e}}n, Peter and Manto, Mario},
doi = {10.1016/B978-0-12-801608-4.00001-3},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boston et al. - 2016 - THE LINGUISTIC CEREBELLUM.pdf:pdf},
isbn = {9780128016084},
title = {{THE LINGUISTIC CEREBELLUM}},
year = {2016}
}
@techreport{Jenkins1992,
abstract = {The finding that expressed emotion is associated with the course of psychiatric disorder has generated a great deal of clinical and research interest in expressed emotion as an important risk factor. Theoretical elucidation of the construct of expressed emotion has lagged considerably behind this interest, however. The authors contribute to a dialogue on what is inside the "black box'' called expressed emotion. They argue that cross-cultural research can provide an empirical basis for the theoretical grounding of expressed emotion factors. A comparative approach reveals that the construct of expressed emotion is essentially cultural in nature. The constellation of emotions, auitudes, and behaviors that are indexed by the expressed emotion method represent cross-culturally variable features of family response to an ill relative. Questions surrounding the cultural validity of the construct of expressed emotion, the qualitative dimensions of expressed emotion, and statistically significant cross-cultural variations in expressed emotion profiles are discussed. Finally, the authors provide an outline of diverse (cul-tural, psychobiological, social-ecological) features of expressed emotion. Anthropological analysis of expressed emotion reveals that although expressed emotion indexes a Pandora's box of diverse features, culture provides the context of variation through which these factors are most productively analyzed.},
author = {Jenkins, Janis H and Karno, Marvin},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenkins, Karno - 1992 - Special Articles The Meaning of Expressed Emotion Theoretical Issues Raised by Cross-Cultural Research.pdf:pdf},
title = {{Special Articles The Meaning of Expressed Emotion: Theoretical Issues Raised by Cross-Cultural Research}},
year = {1992}
}
@article{Vaux2003,
abstract = {This article approaches from a new angle the question of the extent to which predictable information is stored in the lexicon. By examining the ways in which morphological phenomena can be sensitive to prosodic structure, I argue that some - but not all - predictable information is stored in lexical entries. Detailed analysis of a fragment of the Armenian phonological system, focusing on the behavior of consonants at morpheme edges, supports a more abstract view of phonological representations (containing syllables, appendices, and unparsed segments) than can be inferred from phonetic facts alone, contra Ohala and Kawasaki-Fukumori (1997), Steriade (1999), Scheer (2002), and others. The Armenian facts furthermore indicate that attempts to abandon underlying representations (Flemming 1995, Burzio 1996) are misguided and that we must also retreat from the excessively abstract underspecification approaches advocated by most phonologists. {\textcopyright} 2003 by the Massachusetts Institute of Technology.},
author = {Vaux, Bert},
doi = {10.1162/002438903763255931},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Syllabification{\_}In{\_}Armenian{\_}Universal{\_}Gr.pdf:pdf},
issn = {00243892},
journal = {Linguistic Inquiry},
keywords = {Armenian,Lexicon,Morphology-phonology interface,Syllabification},
number = {1},
pages = {91--125},
title = {{Syllabification in Armenian, universal grammar, and the Lexicon}},
volume = {34},
year = {2003}
}
@article{Parada-Cabaleiro2017a,
abstract = {With the increased usage of internet based services and the mass of digital content now available online, the organisation of such content has become a major topic of interest both commercially and within academic research. The addition of emotional understanding for the content is a relevant parameter not only for music classification within digital libraries but also for improving users experiences, via services including automated music recommendation. Despite the singing voice being well-known for the natural communication of emotion, it is still unclear which specific musical characteristics of this signal are involved such affective expressions. The presented study investigates which musical parameters of singing relate to the emotional content, by evaluating the perception of emotion in electronically manipulated a cappella audio samples. A group of 24 individuals participated in a perception test evaluating the emotional dimensions of arousal and valence of 104 sung instances. Key results presented indicate that the rhythmic-melodic contour is potentially related to the perception of arousal whereas musical syntax and tempo can alter the perception of valence.},
author = {Parada-Cabaleiro, Emilia and Baird, Alice and Batliner, Anton and Cummins, Nicholas and Hantke, Simone and Schuller, Bj{\"{o}}rn W.},
doi = {10.1145/3144749.3144756},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro17-TPOa.pdf:pdf},
isbn = {9781450353472},
journal = {ACM International Conference Proceeding Series},
keywords = {A cappella singing,Digital signal processing,Music mood,Perception of emotion},
pages = {29--36},
title = {{The perception of emotion in the singing voice: The understanding of music mood for music organisation}},
year = {2017}
}
@article{Wendt2006,
abstract = {The aim of the present fMRI-study was to investigate the influence of different word prosodies on the activation of the auditory cortex (AC) of 24 subjects. Pseudowords and semantically neutral words were presented with neutral prosody in experiment I and with emotional prosodies in experiment II. We applied two lexical tasks i.e. detecting words or pseudowords. The control task was to detect pure tones. In both studies there was a typical left lateralized activation for speech perception on planum temporale (T3). This territory as part of Wernicke's area is specifically involved in speech perception. A right lateralization simply dependent on prosodic versus neutral content of speech stimuli, as suggested by some literature, is not supported by the current results. In our experiments the emotional information was task-irrelevant and even distracted from the lexical task. Namely, the performance in the detection of words and pseudowords was significantly better in the prosodically neutral condition. Thus, the current results contribute to the clarification of the controversial issue whether prosodies lateralize brain activation to the right, i.e. if lexical rather than prosodic information is in the focus of a task involving prosodic material, a right hemisphere dominance cannot be expected.},
author = {Wendt, Beate and Brechmann, Andr{\'{e}} and Gaschler-Markefski, Birgit and Scheich, Henning and Ackermann, Hermann},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wendt et al. - Unknown - Lateralized processing in human auditory cortex during the perception of emotional prosody.pdf:pdf},
journal = {Proceedings of the 3rd International Conference on Speech Prosody (SP2006)},
pages = {2--5},
title = {{Lateralized processing in human auditory cortex during the perception of emotional prosody}},
url = {http://www.isca-speech.org/archive},
year = {2006}
}
@article{Ross1979,
abstract = {Two patients lost the ability to impart affective qualities to their speech following lesions in the right hemisphere. Arguments are given to support the idea that the right or “minor” hemisphere has a dominant role in modulating the affective components of speech. The anatomical organization of the cortical areas subserving affective speech in the right hemisphere seem to be similar to the organization of cortical areas subserving propositional speech in the left or “major” hemisphere. {\textcopyright} 1979, American Medical Association. All rights reserved.},
author = {Ross, Elliott D. and Mesulam, Marek Marsel},
doi = {10.1001/archneur.1979.00500390062006},
issn = {15383687},
journal = {Archives of Neurology},
number = {3},
pages = {144--148},
title = {{Dominant Language Functions of the Right Hemisphere?: Prosody and Emotional Gesturing}},
volume = {36},
year = {1979}
}
@article{Kraus2005,
abstract = {We have developed a data-driven conceptual framework that links two areas of science: the source-filter model of acoustics and cortical sensory processing streams. The source-filter model describes the mechanics behind speech production: the identity of the speaker is carried largely in the vocal cord source and the message is shaped by the ever-changing filters of the vocal tract. Sensory processing streams, popularly called 'what' and 'where' pathways, are well established in the visual system as a neural scheme for separately carrying different facets of visual objects, namely their identity and their position/motion, to the cortex. A similar functional organization has been postulated in the auditory system. Both speaker identity and the spoken message, which are simultaneously conveyed in the acoustic structure of speech, can be disentangled into discrete brainstem response components. We argue that these two response classes are early manifestations of auditory 'what' and 'where' streams in the cortex. This brainstem link forges a new understanding of the relationship between the acoustics of speech and cortical processing streams, unites two hitherto separate areas in science, and provides a model for future investigations of auditory function. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Kraus, Nina and Nicol, Trent},
doi = {10.1016/j.tins.2005.02.003},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {4},
pages = {176--181},
publisher = {Elsevier Ltd},
title = {{Brainstem origins for cortical 'what' and 'where' pathways in the auditory system}},
volume = {28},
year = {2005}
}
@article{Ohala1984,
abstract = {The author suggests that the following seemingly disparate phenomena have an underlying relationship: (a) cross-language similarities in the intonation contours for statements versus questions, (b) cross-cultural similarities in the vocal expression via intonation of attitude and affect, (c) cross-language patterns in the use of tone, vowels, and consonants in sound symbolic' vocabularly, (d) cross-species use of F0 in threatening or non threatening vocalizations, (e) cross-cultural and cross-species use of certain facial expressions (involving distinct mouth shape), and (f) the existence of sexual dimorphism in the vocal anatomy of humans (and certain non humans). He argues that all arise due to an innately specifiec 'frequency code', which associates high acoustic frequency with the primary meaning of 'small vocalizer' and thus such secondary meaning as 'subordinate, submissive, non threatening, desirous of the receiver's goodwill, etc.' and associates with low acoustic frequency the primary meaning of 'large vocalizer' and such secondary meanings as 'dominant, aggressive, threatening, etc.'},
author = {Ohala, John J.},
doi = {10.1159/000261706},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ohala1984.pdf:pdf},
issn = {00318388},
journal = {Phonetica},
keywords = {249717,No. 1,Phonetica 1984,Vol. 41},
number = {1},
pages = {1--16},
pmid = {6204347},
publisher = {Karger Publishers},
title = {{An ethological perspective on common cross-language utilization of F0 of voice}},
url = {https://www.karger.com/Article/FullText/261706},
volume = {41},
year = {1984}
}
@article{Nevo2001,
abstract = {One hundred and nineteen undergraduate students (62 men and 57 women) of Chinese origin at the National University of Singapore answered three self-report humor questionnaires. Students were also asked to supply their favorite joke (M. A. Johnson, 1991) and a description of a person with an outstanding sense of humor (M. Crawford and D. Gressley, 1991). These responses were compared with results obtained using the same questionnaires and methods in previous studies in Israel and the United States. In general, means and reliabilities of results obtained from the Singapore study replicated those found in other countries. However, Singaporean participants reported significantly less use of humor for coping. Content analysis of jokes supplied by Singaporean students reflected conservative values: Compared with American students, they reported a significantly greater number of jokes with aggressive content and relatively fewer jokes with sexual content. Contrary to expectations, very few gender differences were found. Regardless of gender, a majority of participants nominated a man as an example of a person with an outstanding sense of humor. {\textcopyright} 2001 Taylor {\&} Francis Group, LLC.},
author = {Nevo, Ofra and Nevo, Baruch and Yin, Janie Leong Siew},
doi = {10.1080/00221300109598904},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nevo - 2001 - Singaporean Humor A Cross-Cultural, Cross-Gender Comparison.pdf:pdf},
issn = {19400888},
journal = {Journal of General Psychology},
keywords = {Cultural differences in humor,Gender differences in humor,Singaporean humor},
number = {2},
pages = {143--156},
publisher = {Taylor {\&} Francis Group},
title = {{Singaporean humor: A cross-cultural, cross-gender comparison}},
url = {https://www.researchgate.net/publication/11839116},
volume = {128},
year = {2001}
}
@article{Wendt2003,
author = {Wendt, Beate and Hufnagel, Klaus and Brechmann, Andr{\'{e}} and Gaschler-Markefski, Birgit and Tiedge, J{\"{u}}rgen and Ackermann, Hermann and Scheich, Henning},
doi = {10.1016/s0093-934x(03)00263-3},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wendt et al. - 2003 - A method for creation and validation of a natural spoken language corpus used for prosodic and speech perception.pdf:pdf},
issn = {0093934X},
journal = {Brain and Language},
number = {1},
pages = {187},
title = {{A method for creation and validation of a natural spoken language corpus used for prosodic and speech perception}},
volume = {87},
year = {2003}
}
@inproceedings{Batliner2007,
abstract = {Traditionally, it has been assumed that pitch is the most important prosodic feature for the marking of prominence, and of other phenomena such as the marking of boundaries or emotions. This role has been put into question by recent studies. As nowa-days larger databases are always being processed automatically, it is not clear up to what extent the possibly lower relevance of pitch can be attributed to extraction errors or to other factors. We present some ideas as for a phenomenological difference between pitch and duration, and compare the per-formance of automatically extracted F0 values and of manually corrected F0 values for the automatic recognition of prominence and emotion in sponta-neous speech (children giving commands to a pet robot). The difference in classification performance between corrected and automatically extracted pitch features turns out to be consistent but not very pro-nounced.},
author = {Batliner, A. and Steidl, S. and Schuller, B. and Seppi, D. and Vogt, T. and Devillers, L. and Vidrascu, L. and Amir, N. and Kessous, L. and Aharonson, V.},
booktitle = {Proceedings of ICPhS},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - (PDF) The impact of F0 extraction errors on the classification of prominence and emotion.pdf:pdf},
keywords = {automatic classification,automatic extraction,manual cor-,pitch,rection},
number = {August},
pages = {2201--2204},
title = {{The impact of F0 extraction errors on the classification of prominence and emotion}},
url = {https://www.researchgate.net/publication/338622251{\_}The{\_}impact{\_}of{\_}F0{\_}extraction{\_}errors{\_}on{\_}the{\_}classification{\_}of{\_}prominence{\_}and{\_}emotion http://www.icphs2007.de/conference/Papers/1168/1168.pdf},
year = {2007}
}
@article{Behrens1985,
abstract = {Three separate dichotic listening tasks were run to determine ear superiority for stress identification. When subjects were asked to identify stress placement in real word minimal stress pairs (h{\'{o}}tdog vs. hot d{\'{o}}g), they demonstrated a right ear superiority. When these tokens were filtered so that phonetic and semantic information was eliminated and only the stress pattern remained, a different group of subjects showed a left ear advantage. Finally, with nonsense word counterparts to word stress pairs (b{\'{o}}tgog vs. bot g{\'{o}}g) preserving phonetic information but lacking semantic content, no ear asymmetry was found. These results suggest that as the linguistic significance of the stimuli is reduced, thereby lessening the linguistic function of stress, there is a less dominant involvement of the left hemisphere in stress processing. Results are discussed in relation to a theory of a functional integration of prosodic and segmental speech components that is paralleled by a working partnership of left and right hemisphere. {\textcopyright} 1985.},
author = {Behrens, Susan J.},
doi = {10.1016/0093-934X(85)90047-1},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Behrens - 1985 - The perception of stress and lateralization of prosody.pdf:pdf},
issn = {10902155},
journal = {Brain and Language},
month = {nov},
number = {2},
pages = {332--348},
publisher = {Academic Press},
title = {{The perception of stress and lateralization of prosody}},
volume = {26},
year = {1985}
}
@book{Suqiasyan2004,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
address = {Yerevan},
author = {Nathan, Andrew J. and Scobell, Andrew},
booktitle = {Foreign Affairs},
doi = {10.1017/CBO9781107415324.004},
file = {:home/mhemmer/Downloads/MA-related/Literatur/hayots-lezu-1.pdf:pdf},
isbn = {9781583671993},
issn = {00157120},
keywords = {icle},
number = {5},
pages = {1689--1699},
publisher = {Yerevan State University Publishing House},
title = {{How China sees America}},
url = {https://www.amazon.com/When-Media-Goes-War-Hegemonic-ebook/dp/B00MRWHCYQ},
volume = {91},
year = {2012}
}
@article{Halbertsma2016,
abstract = {Emotions modulate cognitive processes, including those involved in the perception of time. A number of studies have demonstrated that the emotional modulation of interval timing can be described in terms of an attentional or an arousal-based mechanism, depending on the exact task setup. In this paper, two temporal generalization experiments with auditory emotional stimuli as distractors are presented. These experiments are modeled after the work by Lui et al. (PLoS One, 2011, 6, e218292011) who, using visual distractors, provided evidence for an attentional account of emotion-regulated modulation of the perception of time. Experiment 1 replicates the findings of Lui et al., and thus generalizes their work to auditory stimuli. However, Experiment 2, in setup highly similar to Experiment 1, failed to find any effects of emotional modulation on interval timing. These results indicate that emotional effects on interval timing, although often reported, might not be as ubiquitous as earlier research has (implicitly) suggested.},
author = {Halbertsma, Hinke N. and {Van Rijn}, Hedderik},
doi = {10.1163/22134468-00002061},
file = {:home/mhemmer/Downloads/MA-related/Literatur/acoustic cues/HalbertsmavanRijn2016AnEvaluationoftheEffectofAuditoryEmotionalStimulionIntervalTiming.pdf:pdf},
issn = {22134468},
journal = {Timing and Time Perception},
keywords = {Interval timing,attention vs. arousal,auditory stimuli,emotion,pacemaker-accumulator models,temporal modulation,time perception},
number = {1},
pages = {48--62},
title = {{An Evaluation of the Effect of Auditory Emotional Stimuli on Interval Timing}},
volume = {4},
year = {2016}
}
@article{Burkhardt2006,
abstract = {We report on a multilingual comparison study on the effects of prosodic changes on emotional speech. The study was conducted in France, Germany, Greece and Turkey. Semantically identical sentences expressing emotional relevant content were translated into the target languages and were manipulated systematically with respect to pitch range, duration model, and jitter simulation. Perception experiments in the participating countries showed relevant effects irrespective of language. Nonetheless, some effects of language are also reported.},
author = {Burkhardt, F and Audibert, N and Malatesta, L and T{\"{u}}rk, O and Arslan, L and Auberge, V},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burkhardt et al. - 2006 - Emotional Prosody - Does Culture Make A Difference.pdf:pdf},
journal = {Speech Prosody},
number = {1},
pages = {1--4},
title = {{Emotional Prosody - Does Culture Make A Difference?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.148.6152{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2006}
}
@article{Scherer2001a,
abstract = {Whereas the perception of emotion from facial expression has been extensively studied cross-culturally, little is known about judges' ability to infer emotion from vocal cues. This article reports the results from a study conducted in nine countries in Europe, the United States, and Asia on vocal emotion portrayals of anger, sadness, fear, joy, and neutral voice as produced by professional German actors. Data show an overall accuracy of 66{\%} across all emotions and countries. Although accuracy was substantially better than chance, there were sizable differences ranging from 74{\%} in Germany to 52{\%} in Indonesia. However, patterns of confusion were very similar across all countries. These data suggest the existence of similar inference rules from vocal expression across cultures. Generally, accuracy decreased with increasing language dissimilarity from German in spite of the use of language-free speech samples. It is concluded that culture- and language-specific paralinguistic patterns may influence the decoding process. {\textcopyright} 2001 Western Washington University.},
author = {Scherer, Klaus R and Banse, Rainer and Wallbott, Harald G},
doi = {10.1177/0022022101032001009},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scherer et al. - 2001 - EMOTION INFERENCES FROM VOCAL EXPRESSION CORRELATE ACROSS LANGUAGES AND CULTURES(2).pdf:pdf},
issn = {00220221},
journal = {Journal of Cross-Cultural Psychology},
number = {1},
pages = {76--92},
title = {{Emotion inferences from vocal expression correlate across languages and cultures}},
volume = {32},
year = {2001}
}
@article{Draxler2014,
abstract = {In early 2012 the online perception experiment software Percy was deployed on a production server at our lab. Since then, 38 experiments have been made publicly available, with a total of 3078 experiment sessions. In the course of time, the software has been continuously updated and extended to adapt to changing user requirements. Web-based editors for the structure and layout of the experiments have been developed. This paper describes the system architecture, presents usage statistics, discusses typical characteristics of online experiments, and gives an outlook on ongoing work. webapp.phonetik.uni-muenchen.de/WebExperiment lists all currently active experiments.},
author = {Draxler, Christoph},
file = {:home/mhemmer/Downloads/MA-related/Literatur/percyExperiences.pdf:pdf},
isbn = {9782951740884},
journal = {Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014},
keywords = {Online perception experiment,Results,Tool,WWW},
pages = {235--240},
title = {{Online experiments with the Percy software framework - Experiences and some early results}},
year = {2014}
}
@misc{Wildgruber2006,
abstract = {During acoustic communication in humans, information about a speaker's emotional state is predominantly conveyed by modulation of the tone of voice (emotional or affective prosody). Based on lesion data, a right hemisphere superiority for cerebral processing of emotional prosody has been assumed. However, the available clinical studies do not yet provide a coherent picture with respect to interhemispheric lateralization effects of prosody recognition and intrahemispheric localization of the respective brain regions. To further delineate the cerebral network engaged in the perception of emotional tone, a series of experiments was carried out based upon functional magnetic resonance imaging (fMRI). The findings obtained from these investigations allow for the separation of three successive processing stages during recognition of emotional prosody: (1) extraction of suprasegmental acoustic information predominantly subserved by right-sided primary and higher order acoustic regions; (2) representation of meaningful suprasegmental acoustic sequences within posterior aspects of the right superior temporal sulcus; (3) explicit evaluation of emotional prosody at the level of the bilateral inferior frontal cortex. Moreover, implicit processing of affective intonation seems to be bound to subcortical regions mediating automatic induction of specific emotional reactions such as activation of the amygdala in response to fearful stimuli. As concerns lower level processing of the underlying suprasegmental acoustic cues, linguistic and emotional prosody seem to share the same right hemisphere neural resources. Explicit judgment of linguistic aspects of speech prosody, however, appears to be linked to left-sided language areas whereas bilateral orbitofrontal cortex has been found involved in explicit evaluation of emotional prosody. These differences in hemispheric lateralization effects might explain that specific impairments in nonverbal emotional communication subsequent to focal brain lesions are relatively rare clinical observations as compared to the more frequent aphasic disorders. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Wildgruber, D. and Ackermann, H. and Kreifelts, B. and Ethofer, T.},
booktitle = {Progress in Brain Research},
doi = {10.1016/S0079-6123(06)56013-3},
issn = {00796123},
keywords = {affect,communication,emotion,fMRI,intonation,language,lateralization,prosody},
month = {jan},
pages = {249--268},
publisher = {Elsevier},
title = {{Chapter 13 Cerebral processing of linguistic and emotional prosody: fMRI studies}},
volume = {156},
year = {2006}
}
@article{Wendt2002,
abstract = {Previous studies on prosody perception showed inconsistent results concerning the functional role of the hemispheres. One argument for this might be the stimulus material was not sufficiently evaluated. Thus, we developed a corpus which fulfils this requirement. The "Magdeburger Prosodie-Korpus" which was specifically developed for the requirements of fMRI and MEG studies contains two parts. The first part consists of German nouns, the second part of pseudo-words, with both parts being spoken with different emotional prosodies by an actor and an actress. All nouns were evaluated with respect to the emotional connotation (positive, negative, neutral) of their semantic content. The results showed that only a small number of nouns received the same emotional assessment from the participants. The different emotional prosodies of the pseudo-words were assessed by a group of expert (phonetician) and non-expert listeners who are all native German speakers. All emotional prosodies (happiness, sadness, fear, anger, disgust, and neutral), spoken by the man and the woman, were identified by more than 70{\%} of all listeners, except for sadness of the man. There were no significant differences with respect to the gender of the listeners or the speakers. The acoustic analysis showed differences in specific acoustic features of the various emotional prosodies, for example, pitch contour, duration, stress, and intensity. The results were added to the database of the "Magdeburger Prosodie-Korpus".},
author = {Wendt, Beate and Scheich, Henning},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wendt, Scheich - 2002 - The “ Magdeburger Prosodie-Korpus ”.pdf:pdf},
title = {{The “ Magdeburger Prosodie-Korpus ”}},
year = {2002}
}
@article{Matsumoto,
author = {Matsumoto, David},
doi = {10.1177/0022022189201006},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matsumoto - Unknown - Psychology Journal of Cross-Cultural Cultural Influences on the Perception of Emotion On behalf of International A.pdf:pdf},
title = {{Psychology Journal of Cross-Cultural Cultural Influences on the Perception of Emotion On behalf of: International Association for Cross-Cultural Psychology}}
}
@article{Chronaki2018,
abstract = {Humans have an innate set of emotions recognised universally. However, emotion recognition also depends on socio-cultural rules. Although adults recognise vocal emotions universally, they identify emotions more accurately in their native language. We examined developmental trajectories of universal vocal emotion recognition in children. Eighty native English speakers completed a vocal emotion recognition task in their native language (English) and foreign languages (Spanish, Chinese, and Arabic) expressing anger, happiness, sadness, fear, and neutrality. Emotion recognition was compared across 8-to-10, 11-to-13-year-olds, and adults. Measures of behavioural and emotional problems were also taken. Results showed that although emotion recognition was above chance for all languages, native English speaking children were more accurate in recognising vocal emotions in their native language. There was a larger improvement in recognising vocal emotion from the native language during adolescence. Vocal anger recognition did not improve with age for the non-native languages. This is the first study to demonstrate universality of vocal emotion recognition in children whilst supporting an "in-group advantage" for more accurate recognition in the native language. Findings highlight the role of experience in emotion recognition, have implications for child development in modern multicultural societies and address important theoretical questions about the nature of emotions.},
author = {Chronaki, Georgia and Wigelsworth, Michael and Pell, Marc D and Kotz, Sonja A},
doi = {10.1038/s41598-018-26889-1},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chronaki et al. - Unknown - The development of cross-cultural recognition of vocal emotion during childhood and adolescence OPEN.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
title = {{The development of cross-cultural recognition of vocal emotion during childhood and adolescence}},
url = {www.nature.com/scientificreports},
volume = {8},
year = {2018}
}
@article{Pajak2014,
abstract = {The end-result of perceptual reorganization in infancy is currently viewed as a reconfigured perceptual space, "warped" around native-language phonetic categories, which then acts as a direct perceptual filter on any non-native sounds: na{\"{i}}ve-listener discrimination of non-native-sounds is determined by their mapping onto native-language phonetic categories that are acoustically/articulatorily most similar. We report results that suggest another factor in non-native speech perception: some perceptual sensitivities cannot be attributed to listeners' warped perceptual space alone, but rather to enhanced general sensitivity along phonetic dimensions that the listeners' native language employs to distinguish between categories. Specifically, we show that the knowledge of a language with short and long vowel categories leads to enhanced discrimination of non-native consonant length contrasts. We argue that these results support a view of perceptual reorganization as the consequence of learners' hierarchical inductive inferences about the structure of the language's sound system: infants not only acquire the specific phonetic category inventory, but also draw higher-order generalizations over the set of those categories, such as the overall informativity of phonetic dimensions for sound categorization. Non-native sound perception is then also determined by sensitivities that emerge from these generalizations, rather than only by mappings of non-native sounds onto native-language phonetic categories. {\textcopyright} 2014 The Authors.},
author = {Pajak, Bozena and Levy, Roger},
doi = {10.1016/j.wocn.2014.07.001},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pajak, Levy - 2014 - The role of abstraction in non-native speech perception.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Cross-linguistic influence,Inductive inference,Na{\"{i}}ve listeners,Non-native speech perception,Perceptual reorganization,Sound discrimination},
month = {sep},
number = {1},
pages = {147--160},
publisher = {Academic Press},
title = {{The role of abstraction in non-native speech perception}},
volume = {46},
year = {2014}
}
@article{Ackermann2014e,
abstract = {Any account of what is special about the human brain (Passingham 2008) must specify the neural basis of our unique ability to produce speech and delineate how these remarkable motor capabilities could have emerged in our hominin ancestors. Clinical data suggest the basal ganglia provide a platform for the integration of primate-general mechanisms of acoustic communication with the faculty of articulate speech in humans. Furthermore, neurobiological and paleoanthropological data point at a two-stage model of the phylogenetic evolution of this crucial prerequisite of spoken language: (i) monosynaptic refinement of the projections of motor cortex to the brainstem nuclei that steer laryngeal muscles, presumably, as part of a phylogenetic trend associated with increasing brain size during hominin evolution, (ii) subsequent vocal-laryngeal elaboration of cortico-basal ganglia circuitries, driven by human-specific FOXP2 mutations. This concept implies vocal continuity of spoken language evolution at the motor level, elucidating the deep entrenchment of articulate speech into a "nonverbal matrix" (Ingold 1994) which is not accounted for by gestural-origin theories. Moreover, it provides a solution to the question for the adaptive value of the "first word" (Bickerton 2009) since even the earliest and most simple verbal utterances must have increased the versatility of vocal displays afforded by the preceding elaboration of monosynaptic corticobulbar tracts, giving rise to enhanced social cooperation and prestige. At the ontogenetic level, the proposed model assumes age-dependent interactions between the basal ganglia and their cortical targets, similar to vocal learning in some songbirds. In this view, the emergence of articulate speech builds on the "renaissance" of an ancient organizational principle and, hence, may represent an example of evolutionary tinkering (Jacob 1977).},
author = {Ackermann, Hermann and Hage, Steffen R. and Ziegler, Wolfram},
doi = {10.1017/S0140525X13003099},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ackermann, Hage, Ziegler - 2014 - Brain mechanisms of acoustic communication in humans and nonhuman primates An evolutionary perspective.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
keywords = {FOXP2,Human evolution,articulate speech,basal ganglia,speech acquisition,spoken language,striatum,vocal behavior,vocal learning},
month = {may},
pages = {1--84},
pmid = {24827156},
publisher = {Cambridge University Press},
title = {{Brain mechanisms of acoustic communication in humans and nonhuman primates: An evolutionary perspective}},
url = {https://www.researchgate.net/publication/262338913{\_}Brain{\_}mechanisms{\_}of{\_}acoustic{\_}communication{\_}in{\_}humans{\_}and{\_}nonhuman{\_}primates{\_}An{\_}evolutionary{\_}perspective},
volume = {72},
year = {2014}
}
@article{Traunmuller2005,
abstract = {In addition to linguistically coded information, speech conveys necessarily also some paralinguistic information of expressive (affective and adaptive), organic and perspectival kind, but all respective features lack invariant absolute acoustic correlates. According to the Modulation Theory, a speaker's voice functions as a carrier that is modulated by speech gestures. Listeners have to demodulate the signal. Speakers freely vary their voice but compensate for impediments to modulation. Listeners “tune in” to a speech signal based on intrinsic and extrinsic cues and evaluate the deviations of its properties from those they expect of a linguistically neutral vocalization with the same paralinguistic quality. It is shown how this is reflected in the results of various investigations. Most organic and some expressive information is conveyed in the properties of the carrier. Expressive factors affect also amplitude and rate of linguistic modulations. Acquisition and use of speech require a neural linkage between perceptual demodulation and speech motor control (echo neurons). The imitation of body postures and gestures requires analogous structures evidenced in mirror neurons. Relations with gestural theories of speech perception and models of production as well as implications for distinctive feature theory and for the representation of speech in memory are discussed.},
author = {Traunm{\"{u}}ller, Hartmut},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Traunm{\"{u}}ller - Unknown - Speech considered as modulated voice.pdf:pdf},
journal = {Revised Manuscript},
keywords = {acoustic phonetics,behavior,h,imitative,paralinguistic,speech acquisition,speech considered as modulated,speech perception,speech production,theory of speech,traunm{\"{u}}ller,voice},
pages = {1--42},
title = {{Speech considered as modulated voice}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.5988{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@article{Parada-cabaleiro2017a,
author = {Parada-cabaleiro, Emilia and Baird, Alice and Batliner, Anton and Cummins, Nicholas and Hantke, Simone and Schuller, W},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro17-TPO.pdf:pdf},
journal = {Interspeech},
pages = {3246--3250},
title = {{The Perception of Emotions in Noisified Nonsense Speech Chair of Complex and Intelligent Systems , University of Passau , Germany}},
year = {2017}
}
@article{Schirmer2016a,
abstract = {For dynamic sounds, such as vocal expressions, duration often varies alongside speed. Compared to longer sounds, shorter sounds unfold more quickly. Here, we asked whether listeners implicitly use this confound when representing temporal regularities in their environment. In addition, we explored the role of emotions in this process. Using a mismatch negativity (MMN) paradigm, we asked participants to watch a silent movie while passively listening to a stream of task-irrelevant sounds. In Experiment 1, one surprised and one neutral vocalization were compressed and stretched to create stimuli of 378 and 600 ms duration. Stimuli were presented in four blocks, two of which used surprised and two of which used neutral expressions. In one surprised and one neutral block, short and long stimuli served as standards and deviants, respectively. In the other two blocks, the assignment of standards and deviants was reversed. We observed a climbing MMN-like negativity shortly after deviant onset, which suggests that listeners implicitly track sound speed and detect speed changes. Additionally, this MMN-like effect emerged earlier and was larger for long than short deviants, suggesting greater sensitivity to duration increments or slowing down than to decrements or speeding up. Last, deviance detection was facilitated in surprised relative to neutral blocks, indicating that emotion enhances temporal processing. Experiment 2 was comparable to Experiment 1 with the exception that sounds were spectrally rotated to remove vocal emotional content. This abolished the emotional processing benefit, but preserved the other effects. Together, these results provide insights into listener sensitivity to sound speed and raise the possibility that speed biases duration judgements implicitly in a feed-forward manner. Moreover, this bias may be amplified for duration increments relative to decrements and within an emotional relative to a neutral stimulus context.},
author = {Schirmer, Annett and Escoffier, Nicolas and Cheng, Xiaoqin and Feng, Yenju and Penney, Trevor B.},
doi = {10.3389/fpsyg.2015.02055},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirmer et al. - 2016 - Detecting temporal change in dynamic sounds On the role of stimulus duration, speed, and emotion.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Auditory change detection,Event-related potentials,Interval timing,Preattentive,Prosody,Sex differences,Vocal affect},
number = {JAN},
publisher = {Frontiers Media S.A.},
title = {{Detecting temporal change in dynamic sounds: On the role of stimulus duration, speed, and emotion}},
volume = {6},
year = {2016}
}
@article{VanLancker1973a,
abstract = {In past dichotic listening studies, linguistic stimuli have shown a right ear advantage, implying left hemisphere dominance for language processing, while other stimuli incorporating pitch distinctions have shown no ear perference or a left ear (right hemisphere) advantage. Ear preferences in tone language speakers were compared for 3 sets of stimuli: (a) pitch differences within language stimuli (tone-words in the tone language, Thai); (b) language stimuli without pitch differences (consonant-vowel words on mid-tone); and (c) pitch differences alone (hums). Results from 22 native Thai speakers demonstrate that tone-words and consonant-words are better heard at the right ear, while the hums show no ear effect. Preliminary results on 14 English-speaking Ss suggest that the consonant-words give the usual right ear effect, while the tone-words and the hums do not. It is concluded that pitch discrimination is lateralized to the left hemisphere when the pitch differences are linguistically processed. (47 ref.)},
author = {{Van Lancker}, Diana and Fromkin, Victoria A.},
doi = {10.1016/s0095-4470(19)31414-7},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Lancker, Fromkin - 1973 - Hemispheric specialization for pitch and “tone” Evidence from Thai.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
month = {apr},
number = {2},
pages = {101--109},
publisher = {Elsevier BV},
title = {{Hemispheric specialization for pitch and “tone”: Evidence from Thai}},
volume = {1},
year = {1973}
}
@article{Schirmer2004,
abstract = {Time is a fundamental dimension of behavior and as such underlies the perception and production of speech. This paper reviews patient and neuroimaging studies that investigated brain structures that support temporal aspects of speech. The left-frontal cortex, the basal ganglia, and the cerebellum represent structures that have been implicated repeatedly. A comparison with the structures involved in the timing of non-speech events (e.g., tones, lights, finger movements) suggests both commonalities and differences: while the basal ganglia and the cerebellum contribute to the timing of speech and non-speech events, the contribution of left-frontal cortex seems to be specific to speech or rapidly changing acoustic information. Motivated by these commonalities and differences, this paper presents assumptions about the function of basal ganglia, cerebellum, and cortex in the timing of speech. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Schirmer, Annett},
doi = {10.1016/j.cogbrainres.2004.04.003},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/schirmer2004.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Basal ganglia,CLT,Cerebellum,Cognition,Duration,Lateralization,Neural basis of behavior,Prosody,Tempo,VOT},
number = {2},
pages = {269--287},
title = {{Timing speech: A review of lesion and neuroimaging findings}},
volume = {21},
year = {2004}
}
@article{Feldman2013,
abstract = {Infants begin to segment words from fluent speech during the same time period that they learn phonetic categories. Segmented words can provide a potentially useful cue for phonetic learning, yet accounts of phonetic category acquisition typically ignore the contexts in which sounds appear. We present two experiments to show that, contrary to the assumption that phonetic learning occurs in isolation, learners are sensitive to the words in which sounds appear and can use this information to constrain their interpretation of phonetic variability. Experiment 1 shows that adults use word-level information in a phonetic category learning task, assigning acoustically similar vowels to different categories more often when those sounds consistently appear in different words. Experiment 2 demonstrates that 8-month-old infants similarly pay attention to word-level information and that this information affects how they treat phonetic contrasts. These findings suggest that phonetic category learning is a rich, interactive process that takes advantage of many different types of cues that are present in the input. {\textcopyright} 2013 Elsevier B.V.},
author = {Feldman, Naomi H. and Myers, Emily B. and White, Katherine S. and Griffiths, Thomas L. and Morgan, James L.},
doi = {10.1016/j.cognition.2013.02.007},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feldman et al. - 2013 - Word-level information influences phonetic learning in adults and infants.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Language acquisition,Phonetic category learning},
month = {jun},
number = {3},
pages = {427--438},
pmid = {23562941},
title = {{Word-level information influences phonetic learning in adults and infants}},
volume = {127},
year = {2013}
}
@incollection{Best2007,
abstract = {Language experience systematically constrains perception of speech contrasts that deviate phonologically and/or phonetically from those of the listener's native language. These effects are most dramatic in adults, but begin to emerge in infancy and undergo further development through at least early childhood. The central question addressed here is: How do nonnative speech perception findings bear on phonological and phonetic aspects of second language (L2) perceptual learning? A frequent assumption has been that nonnative speech perception can also account for the relative difficulties that late learners have with specific L2 segments and contrasts. However, evaluation of this assumption must take into account the fact that models of nonnative speech perception such as the Perceptual Assimilation Model (PAM) have focused primarily on na{\"{i}}ve listeners, whereas models of L2 speech acquisition such as the Speech Learning Model (SLM) have focused on experienced listeners. This chapter probes the assumption that L2 perceptual learning is determined by nonnative speech perception principles, by considering the commonalities and complementarities between inexperienced listeners and those learning an L2, as viewed from PAM and SLM. Among the issues examined are how language learning may affect perception of phonetic vs. phonological information, how monolingual vs. multiple language experience may impact perception, and what these may imply for attunement of speech perception to changes in the listener's language environment.},
author = {Best, Catherine T. and Tyler, Michael D.},
doi = {10.1075/lllt.17.07bes},
pages = {13--34},
title = {{Nonnative and second-language speech perception}},
year = {2007}
}
@inproceedings{Maganti2007,
author = {Maganti, Hari Krishna and Scherer, Stefan and Palm, G{\"{u}}nther},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-74889-2_62},
isbn = {9783540748885},
issn = {03029743},
pages = {710--711},
title = {{A novel feature for emotion recognition in voice based applications}},
volume = {4738 LNCS},
year = {2007}
}
@article{Gafos2014,
abstract = {We pursue an analysis of the relation between qualitative syllable parses and their quantitative phonetic consequences. To do this, we express the statistics of a symbolic organization corresponding to a syllable parse in terms of continuous phonetic parameters which quantify the timing of the consonants and vowels that make up syllables: consonantal plateau durations, vowel durations, and their variances. These parameters can be estimated from continuous phonetic data. This enables analysis of the link between symbolic phonological form and the continuous phonetics in which this form is manifest. Pursuing such an analysis, we illustrate the predictions of the syllabic organization corresponding to simplex onsets and derive a number of previously experimentally observed and simulation results. Specifically, we derive not only the canonical phonetic manifestations of simplex onsets but also the result that, under certain conditions we make precise, the phonetic indices of the simplex onset organization change to a range of values characteristic of the complex onset organization. Finally, we explore the behavior of phonetic indices for syllabic organization over progressively increasing sizes of lexical samples, thereby concomitantly diversifying the phonetic context over which these indices are taken. {\textcopyright} 2013 Elsevier Ltd.},
author = {Gafos, Adamantios I. and Charlow, Simon and Shaw, Jason A. and Hoole, Philip},
doi = {10.1016/j.wocn.2013.11.007},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gafos et al. - 2014 - Stochastic time analysis of syllable-referential intervals and simplex onsets.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
number = {1},
pages = {152--166},
publisher = {Academic Press},
title = {{Stochastic time analysis of syllable-referential intervals and simplex onsets}},
volume = {44},
year = {2014}
}
@article{Ackermann2004,
abstract = {Besides a sequence of words, spoken utterances are characterized by prosodic (suprasegmental) qualities such as a distinct into-nation contour ("speech melody"), loudness variations, and a rhythmic structure. In addition to a variety of linguistic and pragmatic functions, these features may reflect a speaker's mood and, thus, contribute, concomitant with facial and gestural movements, to the nonverbal expression of emotions (affective prosody). Clinical studies yielded discrepant data on the cerebral correlates of the processing of affective prosody. Functional imaging provides a more recent approach to the analysis of brain-behaviour relationships. The available investigations indicate two successive stages of the perceptual encoding of affective prosody: (a) predominant right-hemisphere processing of intonation contours within posterior parts of the superior temporal gyrus, (b) evaluation of the conveyed emotion at the level of bilateral orbitofrontal cortex. These findings corroborate and extend the model of a more proficient analysis and short-term storage of tonal information within the right cerebral hemisphere.},
author = {Ackermann, H. and Hertrich, I. and Crodd, W. and Wildgruber, D.},
doi = {10.1055/s-2004-828377},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ackermann{\_}Akt{\_}Neurol{\_}31{\_}446{\_}2004.pdf:pdf},
issn = {03024350},
journal = {Aktuelle Neurologie},
number = {9},
pages = {449--460},
title = {{"Das h{\"{o}}ren von gef{\"{u}}hlen": Funktionell-neuro-anatomische grundlagen der verarbeitung affektiver prosodie}},
volume = {31},
year = {2004}
}
@article{Ohala1983,
abstract = {Certain signaling functions of the pitch of voice are remarkably similar across languages and cultures: (1) high or rising pitch to mark questions, low or falling pitch to mark nonquestions; (2) high pitch to signal politeness, low pitch to signal assertiveness; (3) in ‘sound symbolic' vocabulary, high tone used with words connoting smallness or diminutive, low tone with words connoting largeness. These patterns can be explained by the assumption that human vocal communication exploits the ‘frequency code', a cross-species association of high pitch vocalizations with smallness (of the vocalizer), lack of threat, and of low pitch vocalizations with the vocalizer's largeness and threatening intent. {\textcopyright} 1983 S. Karger AG, Basel.},
author = {Ohala, John J.},
doi = {10.1159/000261678},
issn = {1423-0321},
journal = {Phonetica},
keywords = {249722,No. 1,Phonetica 1983,Vol. 40},
number = {1},
pages = {1--18},
publisher = {Karger Publishers},
title = {{Cross-Language Use of Pitch: An Ethological View}},
url = {https://www.karger.com/Article/FullText/261678},
volume = {40},
year = {1983}
}
@article{Draxler2011,
abstract = {Percy is a small software framework for perception experiments via the WWW. It is implemented entirely in dynamic HTML and makes use of the new multimedia tags available in HTML5, eliminating the need for browser plug-ins or external players to display media content. With Percy, perception experiments can be run on any platform supporting HTML5, including tablet computers, smartphones or game consoles and thus access new participant populations. Percy supports touch interfaces and measures reaction times. It stores its data in a relational database system on a server. This allows immediate access to the experiment data via standard database access APIs. The system has been used for a number of experiments in German, Castilian Spanish and English. Copyright {\textcopyright} 2011 ISCA.},
author = {Draxler, Christoph},
file = {:home/mhemmer/Downloads/MA-related/Literatur/percy{\_}short.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {HTML5,Multilingual framework,Online experiment,Reaction times},
pages = {3339--3340},
title = {{Percy - An HTML5 framework for media rich web experiments on mobile devices}},
year = {2011}
}
@article{Poellmann2014,
abstract = {This study investigates if and how listeners adapt to reductions in casual continuous speech. In a perceptual-learning variant of the visual-world paradigm, two groups of Dutch participants were exposed to either segmental (/b/ → [v]) or syllabic (ver- → [f:]) reductions in spoken Dutch sentences. In the test phase, both groups heard both kinds of reductions, but now applied to different words. In one of two experiments, the segmental reduction exposure group was better than the syllabic reduction exposure group in recognizing new reduced /b/-words. In both experiments, the syllabic reduction group showed a greater target preference for new reduced ver-words. Learning about reductions was thus applied to previously unheard words. This lexical generalization suggests that mechanisms compensating for segmental and syllabic reductions take place at a prelexical level, and hence that lexical access involves an abstractionist mode of processing. Existing abstractionist models need to be revised, however, as they do not include representations of sequences of segments (corresponding e.g. to ver-) at the prelexical level. {\textcopyright} 2014 Elsevier Ltd.},
author = {Poellmann, Katja and Bosker, Hans Rutger and McQueen, James M. and Mitterer, Holger},
doi = {10.1016/j.wocn.2014.06.004},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poellmann et al. - 2014 - Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Adaptation,Segmental reduction,Syllabic reduction},
number = {1},
pages = {101--127},
publisher = {Academic Press},
title = {{Perceptual adaptation to segmental and syllabic reductions in continuous spoken Dutch}},
volume = {46},
year = {2014}
}
@article{Banse1996,
abstract = {Professional actors' portrayals of 14 emotions varying in intensity and valence were presented to judges. The results on decoding replicate earlier findings on the ability of judges to infer vocally expressed emotions with much-better-than-chance accuracy, including consistently found differences in the recognizability of different emotions. A total of 224 portrayals were subjected to digital acoustic analysis to obtain profiles of vocal parameters for different emotions. The data suggest that vocal parameters not only index the degree of intensity typical for different emotions but also differentiate valence or quality aspects. The data are also used to test theoretical predictions on vocal patterning based on the component process model of emotion (K. R. Scherer, 1986). Although most hypotheses are supported, some need to be revised on the basis of the empirical evidence. Discriminant analysis and jackknifing show remarkably high hit rates and patterns of confusion that closely mirror those found for listener-judges.},
author = {Banse, Rainer and Scherer, Klaus R.},
doi = {10.1037/0022-3514.70.3.614},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banse, Scherer - 1996 - Acoustic Profiles in Vocal Emotion Expression.pdf:pdf},
issn = {00223514},
journal = {Journal of Personality and Social Psychology},
number = {3},
pages = {614--636},
title = {{Acoustic Profiles in Vocal Emotion Expression}},
volume = {70},
year = {1996}
}
@article{Goldrick2008,
abstract = {Speakers are faster and more accurate at processing certain sound sequences within their language. Does this reflect the fact that these sequences are frequent or that they are phonetically less complex (e.g., easier to articulate)? It has been difficult to contrast these two factors given their high correlation in natural languages. In this study, participants were exposed to novel phonotactic constraints de-correlating complexity and frequency by subjecting the same phonological structure to varying degrees of probabilistic constraint. Participants' behavior was sensitive to variations in frequency, demonstrating that phonotactic probability influences speech production independent of phonetic complexity. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
annote = {Keine kategoriale Trennung zwischen erlaubt und nicht erlaubt. Sondern wie Wahrscheinlich ist es , dass eine Lautkombination in einer Sprache vorkommen kann?},
author = {Goldrick, Matthew and Larson, Meredith},
doi = {10.1016/j.cognition.2007.11.009},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldrick, Larson - Unknown - Phonotactic probability influences speech production.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Markedness,Probabilistic phonotactics,Speech errors,Speech production},
number = {3},
pages = {1155--1164},
title = {{Phonotactic probability influences speech production}},
url = {http://www.elsevier.com/copyright},
volume = {107},
year = {2008}
}
@article{Martirosyan2014,
author = {Martirosyan, Hrach},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Armenian/The{\_}development{\_}of{\_}the{\_}Classical{\_}Armenia.pdf:pdf},
issn = {2306-5737},
journal = {Acta Linguistica Petropolitana},
number = {17 2},
pages = {153--162},
title = {{The development of the Classical Armenian aorist in modern dialects}},
url = {https://www.academia.edu/38064045/The{\_}development{\_}of{\_}the{\_}Classical{\_}Armenian{\_}aorist{\_}in{\_}modern{\_}dialects.{\_}In{\_}Acta{\_}linguistica{\_}Petropolitana{\_}XIV.1{\_}2018{\_}153-162},
volume = {9},
year = {2014}
}
@misc{Berckmoes2004,
abstract = {We review the current state of research on the neural bases of emotional speech perception and raise some issues for future research. First, we situate the key questions in this research by discussing the verbal and vocal channels that constitute the emotional message in spoken language. Second, we glance at four hypotheses regarding where in the brain emotional prosody is processed. Finally, we describe relevant results from neuroimaging studies. We conclude that emotional speech perception is most likely accomplished by a bilateral temporofrontal network with subcortical involvement. The exact contribution of each hemisphere seems to depend on the stimulus features and task demands of research paradigms and remains to be determined.},
author = {Berckmoes, Celine and Vingerhoets, Guy},
booktitle = {Current Directions in Psychological Science},
doi = {10.1111/j.0963-7214.2004.00303.x},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berckmoes, Vingerhoets - Unknown - Neural Foundations of Emotional Speech Processing.pdf:pdf},
issn = {09637214},
keywords = {Affective prosody,Emotional speech,Verbal emotion},
number = {5},
pages = {182--185},
title = {{Neural foundations of emotional speech processing}},
url = {https://about.jstor.org/terms},
volume = {13},
year = {2004}
}
@article{Ye2020,
abstract = {Pitch shifting is a common voice editing technique in which the original pitch of a digital voice is raised or lowered. It is likely to be abused by the malicious attacker to conceal his/her true identity. Existing forensic detection methods are no longer effective for weakly pitch-shifted voice. In this paper, we proposed a convolutional neural network (CNN) to detect not only strongly pitch-shifted voice but also weakly pitch-shifted voice of which the shifting factor is less than ±4 semitones. Specifically, linear frequency cepstral coefficients (LFCC) computed from power spectrums are considered and their dynamic coefficients are extracted as the discriminative features. And the CNN model is carefully designed with particular attention to the input feature map, the activation function and the network topology. We evaluated the algorithm on voices from two datasets with three pitch shifting software. Extensive results show that the algorithm achieves high detection rates for both binary and multiple classifications.},
author = {Ye, Yongchao and Lao, Lingjie and Yan, Diqun and Wang, Rangding},
doi = {10.1155/2020/8927031},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye et al. - 2020 - Identification of Weakly Pitch-Shifted Voice Based on Convolutional Neural Network.pdf:pdf},
issn = {16877586},
journal = {International Journal of Digital Multimedia Broadcasting},
publisher = {Hindawi Limited},
title = {{Identification of Weakly Pitch-Shifted Voice Based on Convolutional Neural Network}},
url = {https://www.researchgate.net/publication/338423791{\_}Identification{\_}of{\_}Weakly{\_}Pitch-Shifted{\_}Voice{\_}Based{\_}on{\_}Convolutional{\_}Neural{\_}Network},
volume = {2020},
year = {2020}
}
@article{Traunmuller1994,
abstract = {Published data on the frequency of the voice fundamental (F0) in speech show its range of variation, often expressed in terms of two standard deviations (SD) of the F0-distribution, to be approximately the same for men and women if expressed in semitones, but the observed SD varies substantially between different investigations. Most of the differences can be attributed to the following factors: SD is increased in tone languages and it varies with the type of discourse. The more ‘lively' the type of discourse, the larger it is. The dependence of SD on the type of discourse tends to be mom pronounced in the speech of women than of men. Based on an analysis of various production data A is shown that speakers normally achieve an increased SD by increasing the excursions of F0 from a ‘base-value' that lies about 1.5 SD below their mean F0. This is relevant to applications in speech technology as well as to general theories of speech communication such as the ‘modulation theory' in which the base-value of F0 is seen as a carrier of frequency.},
author = {Traunm{\"{u}}ller, Hartmut and Eriksson, Anders},
file = {:home/mhemmer/Downloads/MA-related/Literatur/pitch/The{\_}frequency{\_}range{\_}of{\_}the{\_}voice{\_}fundamental{\_}in{\_}th.pdf:pdf},
journal = {Department of Linguistics, University of Stockholm},
pages = {1905191--5},
title = {{The frequency range of the voice fundamental in the speech of male and female adults}},
url = {https://www.researchgate.net/publication/240312210{\_}The{\_}frequency{\_}range{\_}of{\_}the{\_}voice{\_}fundamental{\_}in{\_}the{\_}speech{\_}of{\_}male{\_}and{\_}female{\_}adults},
volume = {97},
year = {1994}
}
@article{Breitenstein2001a,
abstract = {The present study examined acoustic cue utilisation for perception of vocal emotions. Two sets of vocal-emotional stimuli were presented to 35 German and 30 American listeners: (1) sentences in German spoken with five different vocal emotions; and (2) systematically rate- or pitch-altered versions of the original emotional stimuli. In addition to response frequencies on emotional categories, activity ratings were obtained. For the systematically altered stimuli, slow rate was reliably associated with the ‘‘sad'' label. In contrast, fast rate was classified as angry, frightened, or neutral. Manipulation of pitch variation was less potent than rate manipulation in influencing vocal emotional category choices. Reduced pitch variation was associated with perception as sad or neutral; greater pitch variation increased frightened, angry, and happy responses. Performance was highly similar for the two samples, although across tasks, German subjects per- ceived greater variability of activity in the emotional stimuli than did American participants.},
author = {Breitenstein, Caterina and {Van Lancker}, Diana and Daum, Irene},
doi = {10.1080/0269993004200114},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a Ge.pdf:pdf;:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a(2).pdf:pdf;:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a(3).pdf:pdf},
issn = {02699931},
journal = {Cognition and Emotion},
month = {jan},
number = {1},
pages = {57--79},
publisher = {Informa UK Limited},
title = {{The contribution of speech rate and pitch variation to the perception of vocal emotions in a German and an American sample}},
url = {https://www.researchgate.net/publication/247515191{\_}The{\_}contribution{\_}of{\_}speech{\_}rate{\_}and{\_}pitch{\_}variation{\_}to{\_}the{\_}perception{\_}of{\_}vocal{\_}emotions{\_}in{\_}a{\_}German{\_}and{\_}an{\_}American{\_}sample},
volume = {15},
year = {2001}
}
@article{Mucke2014,
abstract = {Research into human communication through the spoken language is full of dichotomies that have often stood in the way of progress in the past, notably the distinction between phonetics and phonology, and more recently, and somewhat orthogonally, between prosody and articulation. The papers collected here make considerable advances in overcoming these restrictions, providing valuable contributions towards the integration of these fields. The increasing evidence for dependencies across the different levels of linguistic structure, and the complexity of the interplay between them, has led to the application of dynamical approaches to spoken language description. With these approaches, coordination and variation within and across systems have begun to play a central role. This paper identifies a common thread through the papers in this issue, in which variation is a consequence of dynamically time-varying behavior that cannot be captured by static snapshots (magic moments). {\textcopyright} 2014 Elsevier Ltd.},
author = {M{\"{u}}cke, Doris and Grice, Martine and Cho, Taehong},
doi = {10.1016/j.wocn.2014.03.001},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\"{u}}cke, Grice, Cho - 2014 - More than a magic moment - Paving the way for dynamics of articulation and prosodic structure†.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Articulation,Dynamical systems,Dynamics of speech,Prosody},
number = {1},
pages = {1--7},
publisher = {Academic Press},
title = {{More than a magic moment - Paving the way for dynamics of articulation and prosodic structure†}},
volume = {44},
year = {2014}
}
@article{Abdel-Hamid2020,
abstract = {Speech emotion recognition (SER) research has usually focused on the analysis of the native language of speakers, most commonly, targeting European and Asian languages. In the present study, a bilingual Arabic/English speech emotion database elicited from 16 male and 16 female Egyptian participants was created in order to investigate how the linguistic and prosodic features were affected by the anger, fear, happiness and sadness emotions across Arabic and English emotional speech. The results of the linguistic analysis indicated that the participants preferred to express their emotions indirectly, mainly using religious references, and that the female participants tended to use language that was more tentative and emotionally expressive, while the male participants tended to use language that was more assertive and independent. As for the prosodic analysis, statistical t-tests showed that the prosodic features of pitch, intensity and speech rate were more indicative of anger and happiness while less relevant to fear and scarcely significant for sadness. Furthermore, speech emotion recognition performed using linear support vector machine (SVM) with AdaBoost also supported these results. In regard to first and second language linguistic features, there was no significant difference in the choice of words and structures expressing the different emotions in the two languages, but in terms of prosodic features, the females' speech showed higher pitch in Arabic in all cases while both genders showed close intensity values in the two languages and faster speech rate in Arabic than in English.},
author = {Abdel-Hamid, Lamiaa and Shaker, Nabil H. and Emara, Ingy},
doi = {10.1109/ACCESS.2020.2987864},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdel-Hamid, Shaker, Emara - 2020 - Analysis of Linguistic and Prosodic Features of Bilingual Arabic-English Speakers for Speech Emotion.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Arabic speech analysis,bilingual linguistic and prosodic features,classification,speech emotion recognition},
pages = {72957--72970},
publisher = {IEEE},
title = {{Analysis of Linguistic and Prosodic Features of Bilingual Arabic-English Speakers for Speech Emotion Recognition}},
volume = {8},
year = {2020}
}
@inproceedings{Esparza2012a,
abstract = {Automatic emotion classification is a task that has been subject of study from very different approaches. Previous research proves that similar performance to humans can be achieved by adequate combination of modalities and features. Nevertheless, large amounts of training data seem necessary to reach a similar level of accurate automatic classification. The labelling of training, validation and test sets is generally a difficult and time consuming task that restricts the experiments. Therefore, in this work we aim at studying self and active training methods and their performance in the task of emotion classification from speech data to reduce annotation costs. The results are compared, using confusion matrices, with the human perception capabilities and supervised training experiments, yielding similar accuracies. {\textcopyright} 2012 Springer-Verlag.},
author = {Esparza, Jos{\'{e}} and Scherer, Stefan and Schwenker, Friedhelm},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-28258-4_3},
isbn = {9783642282577},
issn = {03029743},
keywords = {Human perception of emotion,active learning,automatic emotion classification,emotion recognition from speech,semi-supervised learning},
pages = {19--31},
title = {{Studying self- and active-training methods for multi-feature set emotion recognition}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-28258-4{\_}3},
volume = {7081 LNAI},
year = {2012}
}
@article{Blumstein1974,
abstract = {Two dichotic experiments were conducted to investigate the lateralization of intonation contours. In the first experiment, intonation contours that had been filtered from real speech exemplars of four English sentence types yielded a significant left ear advantage when subjects were given a perceptual matching task. This left ear advantage was maintained when subjects had to identify the same stimuli by their English sentence types. In the second experiment, non-filtered versions of four intonation contours superimposed on a nonsense syllable medium, as well as their filtered equivalents, were presented to subjects, again in a matching task. For both sets of stimuli, a left ear advantage was obtained. Thus, neither the requirements of a linguistic response nor the presence of a phonetic medium succeeded in altering the left ear advantages obtained in the perceptual matching tests. Results from the two experiments suggest that the right hemisphere is directly involved in the perception of intonation contours, and that normal language perception involves the active participation of both cerebral hemispheres. {\textcopyright} 1974, All rights reserved.},
author = {Blumstein, Sheila and Cooper, William E.},
doi = {10.1016/S0010-9452(74)80005-5},
issn = {00109452},
journal = {Cortex},
number = {2},
pages = {146--158},
title = {{Hemispheric Processing of Intonation Contours}},
volume = {10},
year = {1974}
}
@article{Hovhannisyan2015,
abstract = {Prosodic Features of Spoken Utterances in Armenian. – Certain metrical patterns attested in native Armenian spoken utterances are usually labelled as unacceptable in adult standardized speech (ira cf. ir ‘his/her', incha? cf. inch? ‘what?', mezi cf. mez ‘us', etc.) or strange/unaccountable in child language (“Harik senak mət tse” (Tigran 1.2) cf. “Hairiki seniak ʧem mətel”-'I haven't entered daddy's study'). Unfortunately, authentic spoken utterances have not been seriously addressed in Armenian linguistic studies, though as compared to the so- called ‘literary' (standardized and to some extent artificial) speech, naturally spoken utterances often reveal crucial information about the structure of a language or language in general. The present analysis accounts for certain monosyllabic structures surfacing consistently as bisyllabic (C)V.C□′ by adding an obligatory nucleus constituent to the final onset - mez→me.zi ‘us', ir→i.ra ‘his/her'. This reality suggests that the occurrence of augmented units in surface representations is directly dependent on cer- tain phonological processes, those that secure Foot Binarity in prosodic structures to satisfy UG or/and language specific well-formedness conditions for metrical representa- tions.},
author = {Hovhannisyan, Hasmik},
file = {:home/mhemmer/Downloads/MA-related/Literatur/08H{\_}Hovhannisyan.pdf:pdf},
journal = {“Banber”- Bulletin of Yerevan university},
keywords = {armenian prosodic morphology,legality,phonotactics,syllable structure},
mendeley-tags = {armenian prosodic morphology,legality,phonotactics,syllable structure},
pages = {60--70},
title = {{Prosodic Features of Spoken Utterances in Armenian.}},
volume = {№ 3 (18)},
year = {2015}
}
@techreport{Provine1992,
abstract = {The laugh-and/or smile-evoking potency of laughter was evaluated by observing responses of 128 subjects in three undergraduate psychology classes to laugh stimuli produced by a "laugh box." Subjects recorded whether they laughed and/or smiled during each of 10 trials, each of which consisted of an 18-sec sample of laughter, followed by 42 sec of silence. Most subjects laughed and smiled in response to the first presentation oflaughter. However, the polarity of the response changed quickly. By the 10th trial, few subjects laughed and/or smiled, and most found the stimulus "obnoxious." Although other research has described canned-laughter effects, it did not consider the hypothesis confirmed here, that laughter itself evokes laughter, perhaps by activating a laughter-specific auditory-feature detector. This result is relevant to the neurological basis of social communication, human ethology, and theories of speech production and perception.},
author = {Provine, Robert R},
booktitle = {Bulletin of the Psychonomic Society},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Provine - 1992 - Contagious laughter Laughter is a sufficient stimulus for laughs and smiles.pdf:pdf},
number = {1},
pages = {1--4},
title = {{Contagious laughter: Laughter is a sufficient stimulus for laughs and smiles}},
volume = {1992},
year = {1992}
}
@article{Sander2018,
abstract = {This article suggests that methodological and conceptual advancements in affective sciences militate in favor of adopting an appraisal-driven componential approach to further investigate the emotional brain. Here we propose to operationalize this approach by distinguishing five functional networks of the emotional brain: (a) the elicitation network, (b) the expression network, (c) the autonomic reaction network, (d) the action tendency network, and (e) the feeling network, and discuss these networks in the context of the affective neuroscience literature. We also propose that further investigating the “appraising brain” is the royal road to better understand the elicitation network, and may be key to revealing the neural causal mechanisms underlying the emotion process as a whole.},
author = {Sander, David and Grandjean, Didier and Scherer, Klaus R.},
doi = {10.1177/1754073918765653},
file = {:home/mhemmer/Downloads/MA-related/Literatur/SanderGrandjeanSchererEmotionReview2018.pdf:pdf},
issn = {17540739},
journal = {Emotion Review},
keywords = {affective neuroscience,appraisal,brain,emotion},
month = {jul},
number = {3},
pages = {219--231},
publisher = {SAGE Publications Ltd},
title = {{An Appraisal-Driven Componential Approach to the Emotional Brain}},
volume = {10},
year = {2018}
}
@article{Elfenbein2002,
abstract = {A meta-analysis examined emotion recognition within and across cultures. Emotions were universally recognized at better-than-chance levels. Accuracy was higher when emotions were both expressed and recognized by members of the same national, ethnic, or regional group, suggesting an in-group advantage. This advantage was smaller for cultural groups with greater exposure to one another, measured in terms of living in the same nation, physical proximity, and telephone communication. Majority group members were poorer at judging minority group members than the reverse. Cross-cultural accuracy was lower in studies that used a balanced research design, and higher in studies that used imitation rather than posed or spontaneous emotional expressions. Attributes of study design appeared not to moderate the size of the in-group advantage.},
author = {Elfenbein, Hillary Anger and Ambady, Nalini},
doi = {10.1037/0033-2909.128.2.203},
file = {:home/mhemmer/Downloads/MA-related/Literatur/culure/2002ElfenbeinMeta.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
number = {2},
pages = {203--235},
pmid = {11931516},
title = {{On the universality and cultural specificity of emotion recognition: A meta-analysis}},
volume = {128},
year = {2002}
}
@inproceedings{Braun2005,
abstract = {A comparative intercultural study on the so-called basic emotions anger, joy, fear, and sadness as well as neutral utterances was carried out based on samples of dubbed speech. The languages studied were the American English original of a popular TV series (Ally McBeal) as well as its German and Japanese dubbings. The production by the main male and female characters in all three languages as well as the perception by American, German and Japanese listener groups were examined. The present contribution focuses on results on the production and perception side of F0 and related parameters. The principal findings indicate that there are major cultural differences and also gender differences in encoding and decoding the emotional content of the utterances studied. Differences were found to be larger between linguistically and culturally less related languages than between the more closely related ones.},
author = {Braun, Angelika and Katerbow, Matthias},
booktitle = {9th European Conference on Speech Communication and Technology},
pages = {521--524},
title = {{Emotions in dubbed speech: An intercultural approach with respect to F0}},
url = {moz-extension://c20ce5ec-b059-448a-9e27-2947335cb383/enhanced-reader.html?openApp{\&}pdf=https{\%}3A{\%}2F{\%}2Fwww.isca-speech.org{\%}2Farchive{\%}2Farchive{\_}papers{\%}2Finterspeech{\_}2005{\%}2Fi05{\_}0521.pdf},
year = {2005}
}
@article{Keller2013,
abstract = {This article proposes to reconceptualize attachment theory as a culture-sensitive framework. First the seminal contribution of John Bowlby and Mary Ainsworth are recognized in proposing a new paradigm for understanding children's development, synthesizing the interdisciplinary knowledge of relationship formation present during the 1950s (Bowlby) and developing a fieldwork approach in combination with quasi-experimental procedures in the laboratory (Ainsworth). It is argued that students of attachment theory have expanded the framework with respect to the intergenerational transmission and the organizational nature of attachment, relations with psychopathology and clinical applications, and its psychophysiological foundation. It is further argued that attachment theorists were not responsive to developments in evolutionary sciences and cultural/anthropological approaches of parenting and child development. From an evolutionary perspective, contextual variability is crucial to meet the purpose of adaptation. It is demonstrated that the assumptions of monotropy, the conception of stranger anxiety, as well as the definition of attachment in mainstream attachment research are in line with the conception of psychological autonomy, adaptive for Western middle-class, but deviate from the cultural values of many non-Western and mainly rural ecosocial environments. In the concluding paragraph, a strategy is proposed for an empirical research program that would contribute to the cultural foundation of attachment. {\textcopyright} The Author(s) 2013.},
author = {Keller, Heidi},
doi = {10.1177/0022022112472253},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keller - Unknown - Attachment and Culture.pdf:pdf},
issn = {00220221},
journal = {Journal of Cross-Cultural Psychology},
keywords = {cultural psychology,developmental:biological,family/childrearing},
number = {2},
pages = {175--194},
title = {{Attachment and Culture}},
volume = {44},
year = {2013}
}
@article{Weintraub1981,
abstract = {In addition to grammar and semantics, prosody constitutes a third element of speech. Modulations of prosody can produce alterations in the meaning and affective tone of spoken language. Previous studies have suggested that righthemisphere lesions may selectively disrupt a patient's ability to interpret and express the affective component of prosody. On the other hand, this study shows that the effect of right-hemisphere damage on prosody is more widespread. Thus, when discrimination, repetition, and spontaneous production of nonemotional prosody were tested in nine patients with right-sided brain injuries and ten control subjects without brain damage, the patients were found to be significantly worse than the control subjects in their ability to distinguish and express prosodic features that provide phonemic or emphatic information. These results suggest that right-hemisphere damage may affect prosody in a more general manner than was previously assumed. {\textcopyright} 1981, American Medical Association. All rights reserved.},
author = {Weintraub, Sandra and Mesulam, M. Marsel and Kramer, Laura},
doi = {10.1001/archneur.1981.00510120042004},
issn = {15383687},
journal = {Archives of Neurology},
number = {12},
pages = {742--744},
title = {{Disturbances in Prosody: A Right-Hemisphere Contribution to Language}},
volume = {38},
year = {1981}
}
@article{Schwartz2012,
abstract = {This article presents an overview of the Schwartz theory of basic human values. It discusses the nature of values and spells out the features that are common to all values and what distinguishes one value from another. The theory identifies ten basic personal values that are recognized across cultures and explains where they come from. At the heart of the theory is the idea that values form a circular structure that reflects the motivations each value expresses. This circular structure, that captures the conflicts and compatibility among the ten values is apparently culturally universal. The article elucidates the psychological principles that give rise to it. Next, it presents the two major methods developed to measure the basic values, the Schwartz Value Survey and the Portrait Values Questionnaire. Findings from 82 countries, based on these and other methods, provide evidence for the validity of the theory across cultures. The findings reveal substantial differences in the value priorities of individuals. Surprisingly, however, the average value priorities of most societal groups exhibit a similar hierarchical order whose existence the article explains. The last section of the article clarifies how values differ from other concepts used to explain behavior—attitudes, beliefs, norms, and traits.},
author = {Schwartz, Shalom H},
doi = {10.9707/2307-0919.1116},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofstede - 2011 - Unit 2 Theoretical and Methodological Issues Subunit 1 Conceptual Issues in Psychology and Culture Article 8.pdf:pdf},
isbn = {978-0-9845627-0-1},
journal = {Online Readings in Psychology and Culture},
keywords = {Hofsted model,aggregation,national cultures,position of countries},
number = {1},
pages = {12--13},
title = {{An Overview of the Schwartz Theory of Basic Values}},
url = {http://scholarworks.gvsu.edu/orpc/vol2/iss1/8http://scholarworks.gvsu.edu/orpc/vol2/iss1/8 https://doi.org/10.9707/2307-0919.1116},
volume = {2},
year = {2012}
}
@article{Johant1981,
abstract = {The fundamental frequency in speech shows many rapid variations, part of which determine the perceived shape of the pitch contour. This implies that the accuracy with which listeners perceive changes of F0 is more relevant to understanding the perception of intonation than the traditional just noticeable difference of F0 in speech. This study examines the sensitivity to differences in the amount of change of F0, upward (Experiment Ia) and downward (Experiment Ib). Subjects, 74 and 104, respectively, with widely different musical ability can be divided into three categories: (1) Quite a number of them were not able to discriminate differences of less than 4 semitones (nondiscriminators); (2) other subjects wrongly tried to base their judgments on a simple comparison of the final pitches of a stimulus pair (final pitch discriminators); (3) the remaining subjects (pitch distance discriminators) yielded average jnd's of about 1.5 to 2 semitones. Since the issue is associated with musical interval sense, similar experiments were carried out using piano tones. The results were essentially the same as with the speech stimuli. The outcome suggests that only differences of more than 3 semitones play a part in communicative situations. {\textcopyright} 1981, Acoustical Society of America. All rights reserved.},
author = {Johan't, Hart},
doi = {10.1121/1.385592},
issn = {NA},
journal = {Journal of the Acoustical Society of America},
month = {mar},
number = {3},
pages = {811--821},
publisher = {Acoustical Society of America},
title = {{Differential sensitivity to pitch distance, particularly in speech}},
url = {http://asa.scitation.org/doi/10.1121/1.385592},
volume = {69},
year = {1981}
}
@incollection{Kuhl2004,
abstract = {gives a historical perspective on the interactions that have occurred between psychoacoustics and speech perception research as well as a detailed account of her [the author's] recent work on the development of speech prototypes in human infants (PsycINFO Database Record (c) 2012 APA, all rights reserved). (preface)},
author = {Kuhl, Patricia K.},
booktitle = {Developmental psychoacoustics.},
doi = {10.1037/10119-012},
month = {oct},
pages = {293--332},
publisher = {American Psychological Association},
title = {{Psychoacoustics and speech perception: Internal standards, perceptual anchors, and prototypes.}},
year = {2004}
}
