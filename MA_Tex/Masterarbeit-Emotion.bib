Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Wildgruber2006,
abstract = {During acoustic communication in humans, information about a speaker's emotional state is predominantly conveyed by modulation of the tone of voice (emotional or affective prosody). Based on lesion data, a right hemisphere superiority for cerebral processing of emotional prosody has been assumed. However, the available clinical studies do not yet provide a coherent picture with respect to interhemispheric lateralization effects of prosody recognition and intrahemispheric localization of the respective brain regions. To further delineate the cerebral network engaged in the perception of emotional tone, a series of experiments was carried out based upon functional magnetic resonance imaging (fMRI). The findings obtained from these investigations allow for the separation of three successive processing stages during recognition of emotional prosody: (1) extraction of suprasegmental acoustic information predominantly subserved by right-sided primary and higher order acoustic regions; (2) representation of meaningful suprasegmental acoustic sequences within posterior aspects of the right superior temporal sulcus; (3) explicit evaluation of emotional prosody at the level of the bilateral inferior frontal cortex. Moreover, implicit processing of affective intonation seems to be bound to subcortical regions mediating automatic induction of specific emotional reactions such as activation of the amygdala in response to fearful stimuli. As concerns lower level processing of the underlying suprasegmental acoustic cues, linguistic and emotional prosody seem to share the same right hemisphere neural resources. Explicit judgment of linguistic aspects of speech prosody, however, appears to be linked to left-sided language areas whereas bilateral orbitofrontal cortex has been found involved in explicit evaluation of emotional prosody. These differences in hemispheric lateralization effects might explain that specific impairments in nonverbal emotional communication subsequent to focal brain lesions are relatively rare clinical observations as compared to the more frequent aphasic disorders. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Wildgruber, D. and Ackermann, H. and Kreifelts, B. and Ethofer, T.},
booktitle = {Progress in Brain Research},
doi = {10.1016/S0079-6123(06)56013-3},
issn = {00796123},
keywords = {affect,communication,emotion,fMRI,intonation,language,lateralization,prosody},
month = {jan},
pages = {249--268},
publisher = {Elsevier},
title = {{Chapter 13 Cerebral processing of linguistic and emotional prosody: fMRI studies}},
volume = {156},
year = {2006}
}
@article{Curry1967,
abstract = {Twenty-five left-handed and 25 right-handed subjects performed three dichotic listening tasks, two verbal and one non-verbal. Comparisons were made between mean scores obtained at the right and left ears, as well as between the handedness groups. The following results were obtained:1.The mean right ear score was higher than the mean left ear score for both handedness groups on both of the verbal dichotic tasks. This was significant for the right-handed group on both verbal tasks, but for the left-handed group it was significant on only one verbal task.2.The mean left ear score was higher than the mean right ear score for both handedness groups on the non-verbal dichotic task. This betweenears difference was statistically significant for the right-handed group only.3.Comparison of the two handedness groups on each of the three dichotic tests revealed that more left-handed subjects had ear leads which were the reverse of that found for the groups as a whole. This difference between the handedness groups, however, was statistically significant on only one of the dichotic tests.4.The mean left ear score was higher than the mean right ear score for both handedness groups on the non-verbal dichotic task. This betweenears difference was statistically significant for the right-handed group only.5.These data were re-analyzed comparing the size of the absolute between-ears difference scores of those individuals whose ear leads consistently were the reverse of the group as a whole, with subjects who showed no reversals. It was found that the reversal group had smaller mean scores on all three tests, and that this was significant on the two verbal tests. Although more left-handed subjects were found among the reversal group, the data tentatively indicate that the above finding holds true regardless of the handedness of the subjects in the reversal group. The above findings were interpreted as reflecting the different roles of the two cerebral hemispheres, as well as the degree of hemispheric equipotentiality.},
author = {Curry, Frederic K.W.},
doi = {10.1016/s0010-9452(67)80022-4},
issn = {00109452},
journal = {Cortex},
month = {sep},
number = {3},
pages = {343--352},
publisher = {Elsevier BV},
title = {{A Comparison of Left-Handed and Right-Handed Subjects on Verbal and Non-Verbal Dichotic Listening Tasks}},
volume = {3},
year = {1967}
}
@inproceedings{Braun2005,
abstract = {A comparative intercultural study on the so-called basic emotions anger, joy, fear, and sadness as well as neutral utterances was carried out based on samples of dubbed speech. The languages studied were the American English original of a popular TV series (Ally McBeal) as well as its German and Japanese dubbings. The production by the main male and female characters in all three languages as well as the perception by American, German and Japanese listener groups were examined. The present contribution focuses on results on the production and perception side of F0 and related parameters. The principal findings indicate that there are major cultural differences and also gender differences in encoding and decoding the emotional content of the utterances studied. Differences were found to be larger between linguistically and culturally less related languages than between the more closely related ones.},
author = {Braun, Angelika and Katerbow, Matthias},
booktitle = {9th European Conference on Speech Communication and Technology},
pages = {521--524},
title = {{Emotions in dubbed speech: An intercultural approach with respect to F0}},
url = {moz-extension://c20ce5ec-b059-448a-9e27-2947335cb383/enhanced-reader.html?openApp{\&}pdf=https{\%}3A{\%}2F{\%}2Fwww.isca-speech.org{\%}2Farchive{\%}2Farchive{\_}papers{\%}2Finterspeech{\_}2005{\%}2Fi05{\_}0521.pdf},
year = {2005}
}
@article{Pell2020,
abstract = {To investigate the impact of culture on emotion processing, we conducted a study comparing the intensity ratings of external expression (intensity level of speaker's vocal expression) and speaker's internal feeling (intensity level participants think the speaker is experiencing). Specifically, a group of Canadian and Chinese participants categorized emotions (anger, fear, happiness, and sadness) and judged the intensity of emotional utterances in three languages (Chinese, English, and Hindi). Both groups were more accurate at recognizing emotions in their native language. In contrast to related work on facial expressions, which concluded that Eastern participants were more likely to assume that speakers experienced more intense feelings than what they expressed, compared to Western participants, our study on vocal expressions did not find similar cultural effects. Both Canadian and Chinese participants rated the internal feelings of speakers as more intense than their external expressions of emotion. Differences between studies are discussed in terms of the unique structure and social functions of vocal and facial expressions in communication and their interactions with culture.},
author = {Pell, Marc D and Zhang, Shuyi and Pell, Marc D},
doi = {10.13140/RG.2.2.36159.46246},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pell, Zhang, Pell - 2020 - Cross-cultural Differences in Vocal Expression and Emotion Perception Cross-cultural Differences in Vocal Exp.pdf:pdf},
number = {March},
title = {{Cross-cultural Differences in Vocal Expression and Emotion Perception Cross-cultural Differences in Vocal Expression and Emotion Perception}},
url = {https://www.researchgate.net/publication/339988042{\_}Cross-cultural{\_}Differences{\_}in{\_}Vocal{\_}Expression{\_}and{\_}Emotion{\_}Perception},
year = {2020}
}
@article{Sander2018,
abstract = {This article suggests that methodological and conceptual advancements in affective sciences militate in favor of adopting an appraisal-driven componential approach to further investigate the emotional brain. Here we propose to operationalize this approach by distinguishing five functional networks of the emotional brain: (a) the elicitation network, (b) the expression network, (c) the autonomic reaction network, (d) the action tendency network, and (e) the feeling network, and discuss these networks in the context of the affective neuroscience literature. We also propose that further investigating the “appraising brain” is the royal road to better understand the elicitation network, and may be key to revealing the neural causal mechanisms underlying the emotion process as a whole.},
author = {Sander, David and Grandjean, Didier and Scherer, Klaus R.},
doi = {10.1177/1754073918765653},
file = {:home/mhemmer/Downloads/MA-related/Literatur/SanderGrandjeanSchererEmotionReview2018.pdf:pdf},
issn = {17540739},
journal = {Emotion Review},
keywords = {affective neuroscience,appraisal,brain,emotion},
month = {jul},
number = {3},
pages = {219--231},
publisher = {SAGE Publications Ltd},
title = {{An Appraisal-Driven Componential Approach to the Emotional Brain}},
volume = {10},
year = {2018}
}
@article{Weintraub1981,
abstract = {In addition to grammar and semantics, prosody constitutes a third element of speech. Modulations of prosody can produce alterations in the meaning and affective tone of spoken language. Previous studies have suggested that righthemisphere lesions may selectively disrupt a patient's ability to interpret and express the affective component of prosody. On the other hand, this study shows that the effect of right-hemisphere damage on prosody is more widespread. Thus, when discrimination, repetition, and spontaneous production of nonemotional prosody were tested in nine patients with right-sided brain injuries and ten control subjects without brain damage, the patients were found to be significantly worse than the control subjects in their ability to distinguish and express prosodic features that provide phonemic or emphatic information. These results suggest that right-hemisphere damage may affect prosody in a more general manner than was previously assumed. {\textcopyright} 1981, American Medical Association. All rights reserved.},
author = {Weintraub, Sandra and Mesulam, M. Marsel and Kramer, Laura},
doi = {10.1001/archneur.1981.00510120042004},
issn = {15383687},
journal = {Archives of Neurology},
number = {12},
pages = {742--744},
title = {{Disturbances in Prosody: A Right-Hemisphere Contribution to Language}},
volume = {38},
year = {1981}
}
@article{Ross1979,
abstract = {Two patients lost the ability to impart affective qualities to their speech following lesions in the right hemisphere. Arguments are given to support the idea that the right or “minor” hemisphere has a dominant role in modulating the affective components of speech. The anatomical organization of the cortical areas subserving affective speech in the right hemisphere seem to be similar to the organization of cortical areas subserving propositional speech in the left or “major” hemisphere. {\textcopyright} 1979, American Medical Association. All rights reserved.},
author = {Ross, Elliott D. and Mesulam, Marek Marsel},
doi = {10.1001/archneur.1979.00500390062006},
issn = {15383687},
journal = {Archives of Neurology},
number = {3},
pages = {144--148},
title = {{Dominant Language Functions of the Right Hemisphere?: Prosody and Emotional Gesturing}},
volume = {36},
year = {1979}
}
@article{Behrens1985,
abstract = {Three separate dichotic listening tasks were run to determine ear superiority for stress identification. When subjects were asked to identify stress placement in real word minimal stress pairs (h{\'{o}}tdog vs. hot d{\'{o}}g), they demonstrated a right ear superiority. When these tokens were filtered so that phonetic and semantic information was eliminated and only the stress pattern remained, a different group of subjects showed a left ear advantage. Finally, with nonsense word counterparts to word stress pairs (b{\'{o}}tgog vs. bot g{\'{o}}g) preserving phonetic information but lacking semantic content, no ear asymmetry was found. These results suggest that as the linguistic significance of the stimuli is reduced, thereby lessening the linguistic function of stress, there is a less dominant involvement of the left hemisphere in stress processing. Results are discussed in relation to a theory of a functional integration of prosodic and segmental speech components that is paralleled by a working partnership of left and right hemisphere. {\textcopyright} 1985.},
author = {Behrens, Susan J.},
doi = {10.1016/0093-934X(85)90047-1},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Behrens - 1985 - The perception of stress and lateralization of prosody.pdf:pdf},
issn = {10902155},
journal = {Brain and Language},
month = {nov},
number = {2},
pages = {332--348},
publisher = {Academic Press},
title = {{The perception of stress and lateralization of prosody}},
volume = {26},
year = {1985}
}
@article{VanLancker1973a,
abstract = {In past dichotic listening studies, linguistic stimuli have shown a right ear advantage, implying left hemisphere dominance for language processing, while other stimuli incorporating pitch distinctions have shown no ear perference or a left ear (right hemisphere) advantage. Ear preferences in tone language speakers were compared for 3 sets of stimuli: (a) pitch differences within language stimuli (tone-words in the tone language, Thai); (b) language stimuli without pitch differences (consonant-vowel words on mid-tone); and (c) pitch differences alone (hums). Results from 22 native Thai speakers demonstrate that tone-words and consonant-words are better heard at the right ear, while the hums show no ear effect. Preliminary results on 14 English-speaking Ss suggest that the consonant-words give the usual right ear effect, while the tone-words and the hums do not. It is concluded that pitch discrimination is lateralized to the left hemisphere when the pitch differences are linguistically processed. (47 ref.)},
author = {{Van Lancker}, Diana and Fromkin, Victoria A.},
doi = {10.1016/s0095-4470(19)31414-7},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Lancker, Fromkin - 1973 - Hemispheric specialization for pitch and “tone” Evidence from Thai.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
month = {apr},
number = {2},
pages = {101--109},
publisher = {Elsevier BV},
title = {{Hemispheric specialization for pitch and “tone”: Evidence from Thai}},
volume = {1},
year = {1973}
}
@article{Sander2018,
abstract = {Modeling emotion processes remains a conceptual and methodological challenge in affective sciences. In responding to the other target articles in this special section on “Emotion and the Brain” and the comments on our article, we address the issue of potentially separate brain networks subserving the functions of the different emotion components. In particular, we discuss the suggested role of component synchronization in producing information integration for the dynamic emergence of a coherent emotion process, as well as the links between incentive salience (“wanting”) and concern-relevance in the elicitation of emotion.},
author = {Sander, David and Grandjean, Didier and Scherer, Klaus R.},
doi = {10.1177/1754073918783257},
issn = {17540739},
journal = {Emotion Review},
keywords = {emotion components,emotional brain,relevance,synchronization},
month = {jul},
number = {3},
pages = {238--241},
publisher = {SAGE Publications Ltd},
title = {{Brain Networks, Emotion Components, and Appraised Relevance}},
volume = {10},
year = {2018}
}
@article{Ackermann2004,
abstract = {Besides a sequence of words, spoken utterances are characterized by prosodic (suprasegmental) qualities such as a distinct into-nation contour ("speech melody"), loudness variations, and a rhythmic structure. In addition to a variety of linguistic and pragmatic functions, these features may reflect a speaker's mood and, thus, contribute, concomitant with facial and gestural movements, to the nonverbal expression of emotions (affective prosody). Clinical studies yielded discrepant data on the cerebral correlates of the processing of affective prosody. Functional imaging provides a more recent approach to the analysis of brain-behaviour relationships. The available investigations indicate two successive stages of the perceptual encoding of affective prosody: (a) predominant right-hemisphere processing of intonation contours within posterior parts of the superior temporal gyrus, (b) evaluation of the conveyed emotion at the level of bilateral orbitofrontal cortex. These findings corroborate and extend the model of a more proficient analysis and short-term storage of tonal information within the right cerebral hemisphere.},
author = {Ackermann, H. and Hertrich, I. and Crodd, W. and Wildgruber, D.},
doi = {10.1055/s-2004-828377},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ackermann{\_}Akt{\_}Neurol{\_}31{\_}446{\_}2004.pdf:pdf},
issn = {03024350},
journal = {Aktuelle Neurologie},
number = {9},
pages = {449--460},
title = {{"Das h{\"{o}}ren von gef{\"{u}}hlen": Funktionell-neuro-anatomische grundlagen der verarbeitung affektiver prosodie}},
volume = {31},
year = {2004}
}
@article{Droit-Volet2007,
abstract = {Our sense of time is altered by our emotions to such an extent that time seems to fly when we are having fun and drags when we are bored. Recent studies using standardized emotional material provide a unique opportunity for understanding the neurocognitive mechanisms that underlie the effects of emotion on timing and time perception in the milliseconds-to-hours range. We outline how these new findings can be explained within the framework of internal-clock models and describe how emotional arousal and valence interact to produce both increases and decreases in attentional time sharing and clock speed. The study of time and emotion is at a crossroads, and we outline possible examples for future directions. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Droit-Volet, Sylvie and Meck, Warren H.},
doi = {10.1016/j.tics.2007.09.008},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Droit-Volet, Meck - 2007 - How emotions colour our perception of time.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {dec},
number = {12},
pages = {504--513},
pmid = {18023604},
title = {{How emotions colour our perception of time}},
volume = {11},
year = {2007}
}
@article{Parada-Cabaleiro2018a,
abstract = {The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion.},
author = {Parada-Cabaleiro, Emilia and Schmitt, Maximilian and Batliner, Anton and Hantke, Simone and Costantini, Giovanni and Scherer, Klaus and Schuller, Bj{\"{o}}rn W.},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro18-IEI.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
pages = {376--382},
title = {{Identifying emotions in opera singing: Implications of adverse acoustic conditions}},
year = {2018}
}
@techreport{Provine1992,
abstract = {The laugh-and/or smile-evoking potency of laughter was evaluated by observing responses of 128 subjects in three undergraduate psychology classes to laugh stimuli produced by a "laugh box." Subjects recorded whether they laughed and/or smiled during each of 10 trials, each of which consisted of an 18-sec sample of laughter, followed by 42 sec of silence. Most subjects laughed and smiled in response to the first presentation oflaughter. However, the polarity of the response changed quickly. By the 10th trial, few subjects laughed and/or smiled, and most found the stimulus "obnoxious." Although other research has described canned-laughter effects, it did not consider the hypothesis confirmed here, that laughter itself evokes laughter, perhaps by activating a laughter-specific auditory-feature detector. This result is relevant to the neurological basis of social communication, human ethology, and theories of speech production and perception.},
author = {Provine, Robert R},
booktitle = {Bulletin of the Psychonomic Society},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Provine - 1992 - Contagious laughter Laughter is a sufficient stimulus for laughs and smiles.pdf:pdf},
number = {1},
pages = {1--4},
title = {{Contagious laughter: Laughter is a sufficient stimulus for laughs and smiles}},
volume = {1992},
year = {1992}
}
@misc{Schirmer2006,
abstract = {Vocal perception is particularly important for understanding a speaker's emotional state and intentions because, unlike facial perception, it is relatively independent of speaker distance and viewing conditions. The idea, derived from brain lesion studies, that vocal emotional comprehension is a special domain of the right hemisphere has failed to receive consistent support from neuroimaging. This conflict can be reconciled if vocal emotional comprehension is viewed as a multi-step process with individual neural representations. This view reveals a processing chain that proceeds from the ventral auditory pathway to brain structures implicated in cognition and emotion. Thus, vocal emotional comprehension appears to be mediated by bilateral mechanisms anchored within sensory, cognitive and emotional processing systems. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Schirmer, Annett and Kotz, Sonja A.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/j.tics.2005.11.009},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Beyond{\_}the{\_}right{\_}hemisphere{\_}brain{\_}mechan.pdf:pdf},
issn = {13646613},
month = {jan},
number = {1},
pages = {24--30},
title = {{Beyond the right hemisphere: Brain mechanisms mediating vocal emotional processing}},
volume = {10},
year = {2006}
}
@article{Parada-cabaleiro2017a,
author = {Parada-cabaleiro, Emilia and Baird, Alice and Batliner, Anton and Cummins, Nicholas and Hantke, Simone and Schuller, W},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro17-TPO.pdf:pdf},
journal = {Interspeech},
pages = {3246--3250},
title = {{The Perception of Emotions in Noisified Nonsense Speech Chair of Complex and Intelligent Systems , University of Passau , Germany}},
year = {2017}
}
@article{Blumstein1974,
abstract = {Two dichotic experiments were conducted to investigate the lateralization of intonation contours. In the first experiment, intonation contours that had been filtered from real speech exemplars of four English sentence types yielded a significant left ear advantage when subjects were given a perceptual matching task. This left ear advantage was maintained when subjects had to identify the same stimuli by their English sentence types. In the second experiment, non-filtered versions of four intonation contours superimposed on a nonsense syllable medium, as well as their filtered equivalents, were presented to subjects, again in a matching task. For both sets of stimuli, a left ear advantage was obtained. Thus, neither the requirements of a linguistic response nor the presence of a phonetic medium succeeded in altering the left ear advantages obtained in the perceptual matching tests. Results from the two experiments suggest that the right hemisphere is directly involved in the perception of intonation contours, and that normal language perception involves the active participation of both cerebral hemispheres. {\textcopyright} 1974, All rights reserved.},
author = {Blumstein, Sheila and Cooper, William E.},
doi = {10.1016/S0010-9452(74)80005-5},
issn = {00109452},
journal = {Cortex},
number = {2},
pages = {146--158},
title = {{Hemispheric Processing of Intonation Contours}},
volume = {10},
year = {1974}
}
@article{Schirmer2004,
abstract = {Time is a fundamental dimension of behavior and as such underlies the perception and production of speech. This paper reviews patient and neuroimaging studies that investigated brain structures that support temporal aspects of speech. The left-frontal cortex, the basal ganglia, and the cerebellum represent structures that have been implicated repeatedly. A comparison with the structures involved in the timing of non-speech events (e.g., tones, lights, finger movements) suggests both commonalities and differences: while the basal ganglia and the cerebellum contribute to the timing of speech and non-speech events, the contribution of left-frontal cortex seems to be specific to speech or rapidly changing acoustic information. Motivated by these commonalities and differences, this paper presents assumptions about the function of basal ganglia, cerebellum, and cortex in the timing of speech. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Schirmer, Annett},
doi = {10.1016/j.cogbrainres.2004.04.003},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/schirmer2004.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Basal ganglia,CLT,Cerebellum,Cognition,Duration,Lateralization,Neural basis of behavior,Prosody,Tempo,VOT},
number = {2},
pages = {269--287},
title = {{Timing speech: A review of lesion and neuroimaging findings}},
volume = {21},
year = {2004}
}
@article{Baum1982,
abstract = {Three subtests of comprehension (one of varying sentential stress, one of shifting juncture, and one of applying emphatic stress to functors) were administered to eight Broca's aphasics and eight controls to determine the effects of stress and juncture on disambiguation of these sentences. Data indicated that the aphasic group performed significantly worse than the normals on all three subtests of comprehension. In addition, there was a strong positive correlation between severity of aphasia and error scores for all three tests. Results are discussed in relation to intact-vs.-disordered comprehension. {\textcopyright} 1982.},
author = {Baum, Shari R. and Daniloff, Joanne Kelsch and Daniloff, Raymond and Lewis, Jeffrey},
doi = {10.1016/0093-934X(82)90020-7},
issn = {10902155},
journal = {Brain and Language},
number = {2},
pages = {261--271},
title = {{Sentence comprehension by Broca's aphasics: Effects of some suprasegmental variables}},
volume = {17},
year = {1982}
}
@article{Parada-Cabaleiro2017a,
abstract = {With the increased usage of internet based services and the mass of digital content now available online, the organisation of such content has become a major topic of interest both commercially and within academic research. The addition of emotional understanding for the content is a relevant parameter not only for music classification within digital libraries but also for improving users experiences, via services including automated music recommendation. Despite the singing voice being well-known for the natural communication of emotion, it is still unclear which specific musical characteristics of this signal are involved such affective expressions. The presented study investigates which musical parameters of singing relate to the emotional content, by evaluating the perception of emotion in electronically manipulated a cappella audio samples. A group of 24 individuals participated in a perception test evaluating the emotional dimensions of arousal and valence of 104 sung instances. Key results presented indicate that the rhythmic-melodic contour is potentially related to the perception of arousal whereas musical syntax and tempo can alter the perception of valence.},
author = {Parada-Cabaleiro, Emilia and Baird, Alice and Batliner, Anton and Cummins, Nicholas and Hantke, Simone and Schuller, Bj{\"{o}}rn W.},
doi = {10.1145/3144749.3144756},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Parada-Cabaleiro17-TPOa.pdf:pdf},
isbn = {9781450353472},
journal = {ACM International Conference Proceeding Series},
keywords = {A cappella singing,Digital signal processing,Music mood,Perception of emotion},
pages = {29--36},
title = {{The perception of emotion in the singing voice: The understanding of music mood for music organisation}},
year = {2017}
}
@article{Elfenbein2002,
abstract = {A meta-analysis examined emotion recognition within and across cultures. Emotions were universally recognized at better-than-chance levels. Accuracy was higher when emotions were both expressed and recognized by members of the same national, ethnic, or regional group, suggesting an in-group advantage. This advantage was smaller for cultural groups with greater exposure to one another, measured in terms of living in the same nation, physical proximity, and telephone communication. Majority group members were poorer at judging minority group members than the reverse. Cross-cultural accuracy was lower in studies that used a balanced research design, and higher in studies that used imitation rather than posed or spontaneous emotional expressions. Attributes of study design appeared not to moderate the size of the in-group advantage.},
author = {Elfenbein, Hillary Anger and Ambady, Nalini},
doi = {10.1037/0033-2909.128.2.203},
file = {:home/mhemmer/Downloads/MA-related/Literatur/culure/2002ElfenbeinMeta.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
number = {2},
pages = {203--235},
pmid = {11931516},
title = {{On the universality and cultural specificity of emotion recognition: A meta-analysis}},
volume = {128},
year = {2002}
}
