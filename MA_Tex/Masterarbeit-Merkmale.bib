Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Schirmer2019,
abstract = {This study examined how trustworthiness impressions depend on vocal expressive and person characteristics and how their dependence may be explained by acoustical profiles. Sentences spoken in a range of emotional and conversational expressions by 20 speakers differing in age and sex were presented to 80 age and sex matched listeners who rated speaker trustworthiness. Positive speaker valence but not arousal consistently predicted greater perceived trustworthiness. Additionally, voices from younger as compared with older and female as compared with male speakers were judged more trustworthy. Acoustic analysis highlighted several parameters as relevant for differentiating trustworthiness ratings and showed that effects largely overlapped with those for speaker valence and age, but not sex. Specifically, a fast speech rate, a low harmonic-to-noise ratio, and a low fundamental frequency mean and standard deviation differentiated trustworthy from untrustworthy, positive from negative, and younger from older voices. Male and female voices differed in other ways. Together, these results show that a speaker's expressive as well as person characteristics shape trustworthiness impressions and that their effect likely results from a combination of low-level perceptual and higher-order conceptual processes.},
author = {Schirmer, Annett and Feng, Yenju and Sen, Antarika and Penney, Trevor B.},
doi = {10.1371/journal.pone.0210555},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirmer et al. - 2019 - Angry, old, male – and trustworthy How expressive and person voice characteristics shape listener trust.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {jan},
number = {1},
publisher = {Public Library of Science},
title = {{Angry, old, male – and trustworthy? How expressive and person voice characteristics shape listener trust}},
volume = {14},
year = {2019}
}
@misc{Schirmer2006,
abstract = {Vocal perception is particularly important for understanding a speaker's emotional state and intentions because, unlike facial perception, it is relatively independent of speaker distance and viewing conditions. The idea, derived from brain lesion studies, that vocal emotional comprehension is a special domain of the right hemisphere has failed to receive consistent support from neuroimaging. This conflict can be reconciled if vocal emotional comprehension is viewed as a multi-step process with individual neural representations. This view reveals a processing chain that proceeds from the ventral auditory pathway to brain structures implicated in cognition and emotion. Thus, vocal emotional comprehension appears to be mediated by bilateral mechanisms anchored within sensory, cognitive and emotional processing systems. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Schirmer, Annett and Kotz, Sonja A.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/j.tics.2005.11.009},
file = {:home/mhemmer/Downloads/MA-related/Literatur/Beyond{\_}the{\_}right{\_}hemisphere{\_}brain{\_}mechan.pdf:pdf},
issn = {13646613},
month = {jan},
number = {1},
pages = {24--30},
title = {{Beyond the right hemisphere: Brain mechanisms mediating vocal emotional processing}},
volume = {10},
year = {2006}
}
@article{Droit-Volet2007,
abstract = {Our sense of time is altered by our emotions to such an extent that time seems to fly when we are having fun and drags when we are bored. Recent studies using standardized emotional material provide a unique opportunity for understanding the neurocognitive mechanisms that underlie the effects of emotion on timing and time perception in the milliseconds-to-hours range. We outline how these new findings can be explained within the framework of internal-clock models and describe how emotional arousal and valence interact to produce both increases and decreases in attentional time sharing and clock speed. The study of time and emotion is at a crossroads, and we outline possible examples for future directions. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Droit-Volet, Sylvie and Meck, Warren H.},
doi = {10.1016/j.tics.2007.09.008},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Droit-Volet, Meck - 2007 - How emotions colour our perception of time.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {dec},
number = {12},
pages = {504--513},
pmid = {18023604},
title = {{How emotions colour our perception of time}},
volume = {11},
year = {2007}
}
@article{Jadoul2018a,
abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an efficient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Specifically, we focus on applications in data visualisation, file manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
author = {Jadoul, Yannick and Thompson, Bill and de Boer, Bart},
doi = {10.1016/j.wocn.2018.07.001},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Acoustics,Data analysis,Phonetics,Praat,Python,Software},
month = {nov},
pages = {1--15},
publisher = {Academic Press},
title = {{Introducing Parselmouth: A Python interface to Praat}},
volume = {71},
year = {2018}
}
@article{Boston2016,
author = {Boston, Amsterdam • and Heidelberg, • and London, • and York, New and Oxford, • and Paris, • and Diego, San and Mari{\"{e}}n, Peter and Manto, Mario},
doi = {10.1016/B978-0-12-801608-4.00001-3},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boston et al. - 2016 - THE LINGUISTIC CEREBELLUM.pdf:pdf},
isbn = {9780128016084},
title = {{THE LINGUISTIC CEREBELLUM}},
year = {2016}
}
@article{Kraus2005,
abstract = {We have developed a data-driven conceptual framework that links two areas of science: the source-filter model of acoustics and cortical sensory processing streams. The source-filter model describes the mechanics behind speech production: the identity of the speaker is carried largely in the vocal cord source and the message is shaped by the ever-changing filters of the vocal tract. Sensory processing streams, popularly called 'what' and 'where' pathways, are well established in the visual system as a neural scheme for separately carrying different facets of visual objects, namely their identity and their position/motion, to the cortex. A similar functional organization has been postulated in the auditory system. Both speaker identity and the spoken message, which are simultaneously conveyed in the acoustic structure of speech, can be disentangled into discrete brainstem response components. We argue that these two response classes are early manifestations of auditory 'what' and 'where' streams in the cortex. This brainstem link forges a new understanding of the relationship between the acoustics of speech and cortical processing streams, unites two hitherto separate areas in science, and provides a model for future investigations of auditory function. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Kraus, Nina and Nicol, Trent},
doi = {10.1016/j.tins.2005.02.003},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {4},
pages = {176--181},
publisher = {Elsevier Ltd},
title = {{Brainstem origins for cortical 'what' and 'where' pathways in the auditory system}},
volume = {28},
year = {2005}
}
@article{Ohala1984,
abstract = {The author suggests that the following seemingly disparate phenomena have an underlying relationship: (a) cross-language similarities in the intonation contours for statements versus questions, (b) cross-cultural similarities in the vocal expression via intonation of attitude and affect, (c) cross-language patterns in the use of tone, vowels, and consonants in sound symbolic' vocabularly, (d) cross-species use of F0 in threatening or non threatening vocalizations, (e) cross-cultural and cross-species use of certain facial expressions (involving distinct mouth shape), and (f) the existence of sexual dimorphism in the vocal anatomy of humans (and certain non humans). He argues that all arise due to an innately specifiec 'frequency code', which associates high acoustic frequency with the primary meaning of 'small vocalizer' and thus such secondary meaning as 'subordinate, submissive, non threatening, desirous of the receiver's goodwill, etc.' and associates with low acoustic frequency the primary meaning of 'large vocalizer' and such secondary meanings as 'dominant, aggressive, threatening, etc.'},
author = {Ohala, John J.},
doi = {10.1159/000261706},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ohala1984.pdf:pdf},
issn = {00318388},
journal = {Phonetica},
keywords = {249717,No. 1,Phonetica 1984,Vol. 41},
number = {1},
pages = {1--16},
pmid = {6204347},
publisher = {Karger Publishers},
title = {{An ethological perspective on common cross-language utilization of F0 of voice}},
url = {https://www.karger.com/Article/FullText/261706},
volume = {41},
year = {1984}
}
@inproceedings{Batliner2007,
abstract = {Traditionally, it has been assumed that pitch is the most important prosodic feature for the marking of prominence, and of other phenomena such as the marking of boundaries or emotions. This role has been put into question by recent studies. As nowa-days larger databases are always being processed automatically, it is not clear up to what extent the possibly lower relevance of pitch can be attributed to extraction errors or to other factors. We present some ideas as for a phenomenological difference between pitch and duration, and compare the per-formance of automatically extracted F0 values and of manually corrected F0 values for the automatic recognition of prominence and emotion in sponta-neous speech (children giving commands to a pet robot). The difference in classification performance between corrected and automatically extracted pitch features turns out to be consistent but not very pro-nounced.},
author = {Batliner, A. and Steidl, S. and Schuller, B. and Seppi, D. and Vogt, T. and Devillers, L. and Vidrascu, L. and Amir, N. and Kessous, L. and Aharonson, V.},
booktitle = {Proceedings of ICPhS},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - (PDF) The impact of F0 extraction errors on the classification of prominence and emotion.pdf:pdf},
keywords = {automatic classification,automatic extraction,manual cor-,pitch,rection},
number = {August},
pages = {2201--2204},
title = {{The impact of F0 extraction errors on the classification of prominence and emotion}},
url = {https://www.researchgate.net/publication/338622251{\_}The{\_}impact{\_}of{\_}F0{\_}extraction{\_}errors{\_}on{\_}the{\_}classification{\_}of{\_}prominence{\_}and{\_}emotion http://www.icphs2007.de/conference/Papers/1168/1168.pdf},
year = {2007}
}
@article{Halbertsma2016,
abstract = {Emotions modulate cognitive processes, including those involved in the perception of time. A number of studies have demonstrated that the emotional modulation of interval timing can be described in terms of an attentional or an arousal-based mechanism, depending on the exact task setup. In this paper, two temporal generalization experiments with auditory emotional stimuli as distractors are presented. These experiments are modeled after the work by Lui et al. (PLoS One, 2011, 6, e218292011) who, using visual distractors, provided evidence for an attentional account of emotion-regulated modulation of the perception of time. Experiment 1 replicates the findings of Lui et al., and thus generalizes their work to auditory stimuli. However, Experiment 2, in setup highly similar to Experiment 1, failed to find any effects of emotional modulation on interval timing. These results indicate that emotional effects on interval timing, although often reported, might not be as ubiquitous as earlier research has (implicitly) suggested.},
author = {Halbertsma, Hinke N. and {Van Rijn}, Hedderik},
doi = {10.1163/22134468-00002061},
file = {:home/mhemmer/Downloads/MA-related/Literatur/acoustic cues/HalbertsmavanRijn2016AnEvaluationoftheEffectofAuditoryEmotionalStimulionIntervalTiming.pdf:pdf},
issn = {22134468},
journal = {Timing and Time Perception},
keywords = {Interval timing,attention vs. arousal,auditory stimuli,emotion,pacemaker-accumulator models,temporal modulation,time perception},
number = {1},
pages = {48--62},
title = {{An Evaluation of the Effect of Auditory Emotional Stimuli on Interval Timing}},
volume = {4},
year = {2016}
}
@misc{Wildgruber2006,
abstract = {During acoustic communication in humans, information about a speaker's emotional state is predominantly conveyed by modulation of the tone of voice (emotional or affective prosody). Based on lesion data, a right hemisphere superiority for cerebral processing of emotional prosody has been assumed. However, the available clinical studies do not yet provide a coherent picture with respect to interhemispheric lateralization effects of prosody recognition and intrahemispheric localization of the respective brain regions. To further delineate the cerebral network engaged in the perception of emotional tone, a series of experiments was carried out based upon functional magnetic resonance imaging (fMRI). The findings obtained from these investigations allow for the separation of three successive processing stages during recognition of emotional prosody: (1) extraction of suprasegmental acoustic information predominantly subserved by right-sided primary and higher order acoustic regions; (2) representation of meaningful suprasegmental acoustic sequences within posterior aspects of the right superior temporal sulcus; (3) explicit evaluation of emotional prosody at the level of the bilateral inferior frontal cortex. Moreover, implicit processing of affective intonation seems to be bound to subcortical regions mediating automatic induction of specific emotional reactions such as activation of the amygdala in response to fearful stimuli. As concerns lower level processing of the underlying suprasegmental acoustic cues, linguistic and emotional prosody seem to share the same right hemisphere neural resources. Explicit judgment of linguistic aspects of speech prosody, however, appears to be linked to left-sided language areas whereas bilateral orbitofrontal cortex has been found involved in explicit evaluation of emotional prosody. These differences in hemispheric lateralization effects might explain that specific impairments in nonverbal emotional communication subsequent to focal brain lesions are relatively rare clinical observations as compared to the more frequent aphasic disorders. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Wildgruber, D. and Ackermann, H. and Kreifelts, B. and Ethofer, T.},
booktitle = {Progress in Brain Research},
doi = {10.1016/S0079-6123(06)56013-3},
issn = {00796123},
keywords = {affect,communication,emotion,fMRI,intonation,language,lateralization,prosody},
month = {jan},
pages = {249--268},
publisher = {Elsevier},
title = {{Chapter 13 Cerebral processing of linguistic and emotional prosody: fMRI studies}},
volume = {156},
year = {2006}
}
@article{Pajak2014,
abstract = {The end-result of perceptual reorganization in infancy is currently viewed as a reconfigured perceptual space, "warped" around native-language phonetic categories, which then acts as a direct perceptual filter on any non-native sounds: na{\"{i}}ve-listener discrimination of non-native-sounds is determined by their mapping onto native-language phonetic categories that are acoustically/articulatorily most similar. We report results that suggest another factor in non-native speech perception: some perceptual sensitivities cannot be attributed to listeners' warped perceptual space alone, but rather to enhanced general sensitivity along phonetic dimensions that the listeners' native language employs to distinguish between categories. Specifically, we show that the knowledge of a language with short and long vowel categories leads to enhanced discrimination of non-native consonant length contrasts. We argue that these results support a view of perceptual reorganization as the consequence of learners' hierarchical inductive inferences about the structure of the language's sound system: infants not only acquire the specific phonetic category inventory, but also draw higher-order generalizations over the set of those categories, such as the overall informativity of phonetic dimensions for sound categorization. Non-native sound perception is then also determined by sensitivities that emerge from these generalizations, rather than only by mappings of non-native sounds onto native-language phonetic categories. {\textcopyright} 2014 The Authors.},
author = {Pajak, Bozena and Levy, Roger},
doi = {10.1016/j.wocn.2014.07.001},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pajak, Levy - 2014 - The role of abstraction in non-native speech perception.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Cross-linguistic influence,Inductive inference,Na{\"{i}}ve listeners,Non-native speech perception,Perceptual reorganization,Sound discrimination},
month = {sep},
number = {1},
pages = {147--160},
publisher = {Academic Press},
title = {{The role of abstraction in non-native speech perception}},
volume = {46},
year = {2014}
}
@article{Ackermann2014e,
abstract = {Any account of what is special about the human brain (Passingham 2008) must specify the neural basis of our unique ability to produce speech and delineate how these remarkable motor capabilities could have emerged in our hominin ancestors. Clinical data suggest the basal ganglia provide a platform for the integration of primate-general mechanisms of acoustic communication with the faculty of articulate speech in humans. Furthermore, neurobiological and paleoanthropological data point at a two-stage model of the phylogenetic evolution of this crucial prerequisite of spoken language: (i) monosynaptic refinement of the projections of motor cortex to the brainstem nuclei that steer laryngeal muscles, presumably, as part of a phylogenetic trend associated with increasing brain size during hominin evolution, (ii) subsequent vocal-laryngeal elaboration of cortico-basal ganglia circuitries, driven by human-specific FOXP2 mutations. This concept implies vocal continuity of spoken language evolution at the motor level, elucidating the deep entrenchment of articulate speech into a "nonverbal matrix" (Ingold 1994) which is not accounted for by gestural-origin theories. Moreover, it provides a solution to the question for the adaptive value of the "first word" (Bickerton 2009) since even the earliest and most simple verbal utterances must have increased the versatility of vocal displays afforded by the preceding elaboration of monosynaptic corticobulbar tracts, giving rise to enhanced social cooperation and prestige. At the ontogenetic level, the proposed model assumes age-dependent interactions between the basal ganglia and their cortical targets, similar to vocal learning in some songbirds. In this view, the emergence of articulate speech builds on the "renaissance" of an ancient organizational principle and, hence, may represent an example of evolutionary tinkering (Jacob 1977).},
author = {Ackermann, Hermann and Hage, Steffen R. and Ziegler, Wolfram},
doi = {10.1017/S0140525X13003099},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ackermann, Hage, Ziegler - 2014 - Brain mechanisms of acoustic communication in humans and nonhuman primates An evolutionary perspective.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
keywords = {FOXP2,Human evolution,articulate speech,basal ganglia,speech acquisition,spoken language,striatum,vocal behavior,vocal learning},
month = {may},
pages = {1--84},
pmid = {24827156},
publisher = {Cambridge University Press},
title = {{Brain mechanisms of acoustic communication in humans and nonhuman primates: An evolutionary perspective}},
url = {https://www.researchgate.net/publication/262338913{\_}Brain{\_}mechanisms{\_}of{\_}acoustic{\_}communication{\_}in{\_}humans{\_}and{\_}nonhuman{\_}primates{\_}An{\_}evolutionary{\_}perspective},
volume = {72},
year = {2014}
}
@article{Traunmuller2005,
abstract = {In addition to linguistically coded information, speech conveys necessarily also some paralinguistic information of expressive (affective and adaptive), organic and perspectival kind, but all respective features lack invariant absolute acoustic correlates. According to the Modulation Theory, a speaker's voice functions as a carrier that is modulated by speech gestures. Listeners have to demodulate the signal. Speakers freely vary their voice but compensate for impediments to modulation. Listeners “tune in” to a speech signal based on intrinsic and extrinsic cues and evaluate the deviations of its properties from those they expect of a linguistically neutral vocalization with the same paralinguistic quality. It is shown how this is reflected in the results of various investigations. Most organic and some expressive information is conveyed in the properties of the carrier. Expressive factors affect also amplitude and rate of linguistic modulations. Acquisition and use of speech require a neural linkage between perceptual demodulation and speech motor control (echo neurons). The imitation of body postures and gestures requires analogous structures evidenced in mirror neurons. Relations with gestural theories of speech perception and models of production as well as implications for distinctive feature theory and for the representation of speech in memory are discussed.},
author = {Traunm{\"{u}}ller, Hartmut},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Traunm{\"{u}}ller - Unknown - Speech considered as modulated voice.pdf:pdf},
journal = {Revised Manuscript},
keywords = {acoustic phonetics,behavior,h,imitative,paralinguistic,speech acquisition,speech considered as modulated,speech perception,speech production,theory of speech,traunm{\"{u}}ller,voice},
pages = {1--42},
title = {{Speech considered as modulated voice}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.5988{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@article{Schirmer2016a,
abstract = {For dynamic sounds, such as vocal expressions, duration often varies alongside speed. Compared to longer sounds, shorter sounds unfold more quickly. Here, we asked whether listeners implicitly use this confound when representing temporal regularities in their environment. In addition, we explored the role of emotions in this process. Using a mismatch negativity (MMN) paradigm, we asked participants to watch a silent movie while passively listening to a stream of task-irrelevant sounds. In Experiment 1, one surprised and one neutral vocalization were compressed and stretched to create stimuli of 378 and 600 ms duration. Stimuli were presented in four blocks, two of which used surprised and two of which used neutral expressions. In one surprised and one neutral block, short and long stimuli served as standards and deviants, respectively. In the other two blocks, the assignment of standards and deviants was reversed. We observed a climbing MMN-like negativity shortly after deviant onset, which suggests that listeners implicitly track sound speed and detect speed changes. Additionally, this MMN-like effect emerged earlier and was larger for long than short deviants, suggesting greater sensitivity to duration increments or slowing down than to decrements or speeding up. Last, deviance detection was facilitated in surprised relative to neutral blocks, indicating that emotion enhances temporal processing. Experiment 2 was comparable to Experiment 1 with the exception that sounds were spectrally rotated to remove vocal emotional content. This abolished the emotional processing benefit, but preserved the other effects. Together, these results provide insights into listener sensitivity to sound speed and raise the possibility that speed biases duration judgements implicitly in a feed-forward manner. Moreover, this bias may be amplified for duration increments relative to decrements and within an emotional relative to a neutral stimulus context.},
author = {Schirmer, Annett and Escoffier, Nicolas and Cheng, Xiaoqin and Feng, Yenju and Penney, Trevor B.},
doi = {10.3389/fpsyg.2015.02055},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirmer et al. - 2016 - Detecting temporal change in dynamic sounds On the role of stimulus duration, speed, and emotion.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Auditory change detection,Event-related potentials,Interval timing,Preattentive,Prosody,Sex differences,Vocal affect},
number = {JAN},
publisher = {Frontiers Media S.A.},
title = {{Detecting temporal change in dynamic sounds: On the role of stimulus duration, speed, and emotion}},
volume = {6},
year = {2016}
}
@article{VanLancker1973a,
abstract = {In past dichotic listening studies, linguistic stimuli have shown a right ear advantage, implying left hemisphere dominance for language processing, while other stimuli incorporating pitch distinctions have shown no ear perference or a left ear (right hemisphere) advantage. Ear preferences in tone language speakers were compared for 3 sets of stimuli: (a) pitch differences within language stimuli (tone-words in the tone language, Thai); (b) language stimuli without pitch differences (consonant-vowel words on mid-tone); and (c) pitch differences alone (hums). Results from 22 native Thai speakers demonstrate that tone-words and consonant-words are better heard at the right ear, while the hums show no ear effect. Preliminary results on 14 English-speaking Ss suggest that the consonant-words give the usual right ear effect, while the tone-words and the hums do not. It is concluded that pitch discrimination is lateralized to the left hemisphere when the pitch differences are linguistically processed. (47 ref.)},
author = {{Van Lancker}, Diana and Fromkin, Victoria A.},
doi = {10.1016/s0095-4470(19)31414-7},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Lancker, Fromkin - 1973 - Hemispheric specialization for pitch and “tone” Evidence from Thai.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
month = {apr},
number = {2},
pages = {101--109},
publisher = {Elsevier BV},
title = {{Hemispheric specialization for pitch and “tone”: Evidence from Thai}},
volume = {1},
year = {1973}
}
@article{Schirmer2004,
abstract = {Time is a fundamental dimension of behavior and as such underlies the perception and production of speech. This paper reviews patient and neuroimaging studies that investigated brain structures that support temporal aspects of speech. The left-frontal cortex, the basal ganglia, and the cerebellum represent structures that have been implicated repeatedly. A comparison with the structures involved in the timing of non-speech events (e.g., tones, lights, finger movements) suggests both commonalities and differences: while the basal ganglia and the cerebellum contribute to the timing of speech and non-speech events, the contribution of left-frontal cortex seems to be specific to speech or rapidly changing acoustic information. Motivated by these commonalities and differences, this paper presents assumptions about the function of basal ganglia, cerebellum, and cortex in the timing of speech. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Schirmer, Annett},
doi = {10.1016/j.cogbrainres.2004.04.003},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/schirmer2004.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Basal ganglia,CLT,Cerebellum,Cognition,Duration,Lateralization,Neural basis of behavior,Prosody,Tempo,VOT},
number = {2},
pages = {269--287},
title = {{Timing speech: A review of lesion and neuroimaging findings}},
volume = {21},
year = {2004}
}
@incollection{Best2007,
abstract = {Language experience systematically constrains perception of speech contrasts that deviate phonologically and/or phonetically from those of the listener's native language. These effects are most dramatic in adults, but begin to emerge in infancy and undergo further development through at least early childhood. The central question addressed here is: How do nonnative speech perception findings bear on phonological and phonetic aspects of second language (L2) perceptual learning? A frequent assumption has been that nonnative speech perception can also account for the relative difficulties that late learners have with specific L2 segments and contrasts. However, evaluation of this assumption must take into account the fact that models of nonnative speech perception such as the Perceptual Assimilation Model (PAM) have focused primarily on na{\"{i}}ve listeners, whereas models of L2 speech acquisition such as the Speech Learning Model (SLM) have focused on experienced listeners. This chapter probes the assumption that L2 perceptual learning is determined by nonnative speech perception principles, by considering the commonalities and complementarities between inexperienced listeners and those learning an L2, as viewed from PAM and SLM. Among the issues examined are how language learning may affect perception of phonetic vs. phonological information, how monolingual vs. multiple language experience may impact perception, and what these may imply for attunement of speech perception to changes in the listener's language environment.},
author = {Best, Catherine T. and Tyler, Michael D.},
doi = {10.1075/lllt.17.07bes},
pages = {13--34},
title = {{Nonnative and second-language speech perception}},
year = {2007}
}
@article{Ackermann2004,
abstract = {Besides a sequence of words, spoken utterances are characterized by prosodic (suprasegmental) qualities such as a distinct into-nation contour ("speech melody"), loudness variations, and a rhythmic structure. In addition to a variety of linguistic and pragmatic functions, these features may reflect a speaker's mood and, thus, contribute, concomitant with facial and gestural movements, to the nonverbal expression of emotions (affective prosody). Clinical studies yielded discrepant data on the cerebral correlates of the processing of affective prosody. Functional imaging provides a more recent approach to the analysis of brain-behaviour relationships. The available investigations indicate two successive stages of the perceptual encoding of affective prosody: (a) predominant right-hemisphere processing of intonation contours within posterior parts of the superior temporal gyrus, (b) evaluation of the conveyed emotion at the level of bilateral orbitofrontal cortex. These findings corroborate and extend the model of a more proficient analysis and short-term storage of tonal information within the right cerebral hemisphere.},
author = {Ackermann, H. and Hertrich, I. and Crodd, W. and Wildgruber, D.},
doi = {10.1055/s-2004-828377},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ackermann{\_}Akt{\_}Neurol{\_}31{\_}446{\_}2004.pdf:pdf},
issn = {03024350},
journal = {Aktuelle Neurologie},
number = {9},
pages = {449--460},
title = {{"Das h{\"{o}}ren von gef{\"{u}}hlen": Funktionell-neuro-anatomische grundlagen der verarbeitung affektiver prosodie}},
volume = {31},
year = {2004}
}
@article{Ohala1983,
abstract = {Certain signaling functions of the pitch of voice are remarkably similar across languages and cultures: (1) high or rising pitch to mark questions, low or falling pitch to mark nonquestions; (2) high pitch to signal politeness, low pitch to signal assertiveness; (3) in ‘sound symbolic' vocabulary, high tone used with words connoting smallness or diminutive, low tone with words connoting largeness. These patterns can be explained by the assumption that human vocal communication exploits the ‘frequency code', a cross-species association of high pitch vocalizations with smallness (of the vocalizer), lack of threat, and of low pitch vocalizations with the vocalizer's largeness and threatening intent. {\textcopyright} 1983 S. Karger AG, Basel.},
author = {Ohala, John J.},
doi = {10.1159/000261678},
issn = {1423-0321},
journal = {Phonetica},
keywords = {249722,No. 1,Phonetica 1983,Vol. 40},
number = {1},
pages = {1--18},
publisher = {Karger Publishers},
title = {{Cross-Language Use of Pitch: An Ethological View}},
url = {https://www.karger.com/Article/FullText/261678},
volume = {40},
year = {1983}
}
@article{Banse1996,
abstract = {Professional actors' portrayals of 14 emotions varying in intensity and valence were presented to judges. The results on decoding replicate earlier findings on the ability of judges to infer vocally expressed emotions with much-better-than-chance accuracy, including consistently found differences in the recognizability of different emotions. A total of 224 portrayals were subjected to digital acoustic analysis to obtain profiles of vocal parameters for different emotions. The data suggest that vocal parameters not only index the degree of intensity typical for different emotions but also differentiate valence or quality aspects. The data are also used to test theoretical predictions on vocal patterning based on the component process model of emotion (K. R. Scherer, 1986). Although most hypotheses are supported, some need to be revised on the basis of the empirical evidence. Discriminant analysis and jackknifing show remarkably high hit rates and patterns of confusion that closely mirror those found for listener-judges.},
author = {Banse, Rainer and Scherer, Klaus R.},
doi = {10.1037/0022-3514.70.3.614},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banse, Scherer - 1996 - Acoustic Profiles in Vocal Emotion Expression.pdf:pdf},
issn = {00223514},
journal = {Journal of Personality and Social Psychology},
number = {3},
pages = {614--636},
title = {{Acoustic Profiles in Vocal Emotion Expression}},
volume = {70},
year = {1996}
}
@article{Ye2020,
abstract = {Pitch shifting is a common voice editing technique in which the original pitch of a digital voice is raised or lowered. It is likely to be abused by the malicious attacker to conceal his/her true identity. Existing forensic detection methods are no longer effective for weakly pitch-shifted voice. In this paper, we proposed a convolutional neural network (CNN) to detect not only strongly pitch-shifted voice but also weakly pitch-shifted voice of which the shifting factor is less than ±4 semitones. Specifically, linear frequency cepstral coefficients (LFCC) computed from power spectrums are considered and their dynamic coefficients are extracted as the discriminative features. And the CNN model is carefully designed with particular attention to the input feature map, the activation function and the network topology. We evaluated the algorithm on voices from two datasets with three pitch shifting software. Extensive results show that the algorithm achieves high detection rates for both binary and multiple classifications.},
author = {Ye, Yongchao and Lao, Lingjie and Yan, Diqun and Wang, Rangding},
doi = {10.1155/2020/8927031},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye et al. - 2020 - Identification of Weakly Pitch-Shifted Voice Based on Convolutional Neural Network.pdf:pdf},
issn = {16877586},
journal = {International Journal of Digital Multimedia Broadcasting},
publisher = {Hindawi Limited},
title = {{Identification of Weakly Pitch-Shifted Voice Based on Convolutional Neural Network}},
url = {https://www.researchgate.net/publication/338423791{\_}Identification{\_}of{\_}Weakly{\_}Pitch-Shifted{\_}Voice{\_}Based{\_}on{\_}Convolutional{\_}Neural{\_}Network},
volume = {2020},
year = {2020}
}
@article{Traunmuller1994,
abstract = {Published data on the frequency of the voice fundamental (F0) in speech show its range of variation, often expressed in terms of two standard deviations (SD) of the F0-distribution, to be approximately the same for men and women if expressed in semitones, but the observed SD varies substantially between different investigations. Most of the differences can be attributed to the following factors: SD is increased in tone languages and it varies with the type of discourse. The more ‘lively' the type of discourse, the larger it is. The dependence of SD on the type of discourse tends to be mom pronounced in the speech of women than of men. Based on an analysis of various production data A is shown that speakers normally achieve an increased SD by increasing the excursions of F0 from a ‘base-value' that lies about 1.5 SD below their mean F0. This is relevant to applications in speech technology as well as to general theories of speech communication such as the ‘modulation theory' in which the base-value of F0 is seen as a carrier of frequency.},
author = {Traunm{\"{u}}ller, Hartmut and Eriksson, Anders},
file = {:home/mhemmer/Downloads/MA-related/Literatur/pitch/The{\_}frequency{\_}range{\_}of{\_}the{\_}voice{\_}fundamental{\_}in{\_}th.pdf:pdf},
journal = {Department of Linguistics, University of Stockholm},
pages = {1905191--5},
title = {{The frequency range of the voice fundamental in the speech of male and female adults}},
url = {https://www.researchgate.net/publication/240312210{\_}The{\_}frequency{\_}range{\_}of{\_}the{\_}voice{\_}fundamental{\_}in{\_}the{\_}speech{\_}of{\_}male{\_}and{\_}female{\_}adults},
volume = {97},
year = {1994}
}
@article{Breitenstein2001a,
abstract = {The present study examined acoustic cue utilisation for perception of vocal emotions. Two sets of vocal-emotional stimuli were presented to 35 German and 30 American listeners: (1) sentences in German spoken with five different vocal emotions; and (2) systematically rate- or pitch-altered versions of the original emotional stimuli. In addition to response frequencies on emotional categories, activity ratings were obtained. For the systematically altered stimuli, slow rate was reliably associated with the ‘‘sad'' label. In contrast, fast rate was classified as angry, frightened, or neutral. Manipulation of pitch variation was less potent than rate manipulation in influencing vocal emotional category choices. Reduced pitch variation was associated with perception as sad or neutral; greater pitch variation increased frightened, angry, and happy responses. Performance was highly similar for the two samples, although across tasks, German subjects per- ceived greater variability of activity in the emotional stimuli than did American participants.},
author = {Breitenstein, Caterina and {Van Lancker}, Diana and Daum, Irene},
doi = {10.1080/0269993004200114},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a Ge.pdf:pdf;:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a(2).pdf:pdf;:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breitenstein, Van Lancker, Daum - 2001 - The contribution of speech rate and pitch variation to the perception of vocal emotions in a(3).pdf:pdf},
issn = {02699931},
journal = {Cognition and Emotion},
month = {jan},
number = {1},
pages = {57--79},
publisher = {Informa UK Limited},
title = {{The contribution of speech rate and pitch variation to the perception of vocal emotions in a German and an American sample}},
url = {https://www.researchgate.net/publication/247515191{\_}The{\_}contribution{\_}of{\_}speech{\_}rate{\_}and{\_}pitch{\_}variation{\_}to{\_}the{\_}perception{\_}of{\_}vocal{\_}emotions{\_}in{\_}a{\_}German{\_}and{\_}an{\_}American{\_}sample},
volume = {15},
year = {2001}
}
@inproceedings{Braun2005,
abstract = {A comparative intercultural study on the so-called basic emotions anger, joy, fear, and sadness as well as neutral utterances was carried out based on samples of dubbed speech. The languages studied were the American English original of a popular TV series (Ally McBeal) as well as its German and Japanese dubbings. The production by the main male and female characters in all three languages as well as the perception by American, German and Japanese listener groups were examined. The present contribution focuses on results on the production and perception side of F0 and related parameters. The principal findings indicate that there are major cultural differences and also gender differences in encoding and decoding the emotional content of the utterances studied. Differences were found to be larger between linguistically and culturally less related languages than between the more closely related ones.},
author = {Braun, Angelika and Katerbow, Matthias},
booktitle = {9th European Conference on Speech Communication and Technology},
pages = {521--524},
title = {{Emotions in dubbed speech: An intercultural approach with respect to F0}},
url = {moz-extension://c20ce5ec-b059-448a-9e27-2947335cb383/enhanced-reader.html?openApp{\&}pdf=https{\%}3A{\%}2F{\%}2Fwww.isca-speech.org{\%}2Farchive{\%}2Farchive{\_}papers{\%}2Finterspeech{\_}2005{\%}2Fi05{\_}0521.pdf},
year = {2005}
}
@article{Johant1981,
abstract = {The fundamental frequency in speech shows many rapid variations, part of which determine the perceived shape of the pitch contour. This implies that the accuracy with which listeners perceive changes of F0 is more relevant to understanding the perception of intonation than the traditional just noticeable difference of F0 in speech. This study examines the sensitivity to differences in the amount of change of F0, upward (Experiment Ia) and downward (Experiment Ib). Subjects, 74 and 104, respectively, with widely different musical ability can be divided into three categories: (1) Quite a number of them were not able to discriminate differences of less than 4 semitones (nondiscriminators); (2) other subjects wrongly tried to base their judgments on a simple comparison of the final pitches of a stimulus pair (final pitch discriminators); (3) the remaining subjects (pitch distance discriminators) yielded average jnd's of about 1.5 to 2 semitones. Since the issue is associated with musical interval sense, similar experiments were carried out using piano tones. The results were essentially the same as with the speech stimuli. The outcome suggests that only differences of more than 3 semitones play a part in communicative situations. {\textcopyright} 1981, Acoustical Society of America. All rights reserved.},
author = {Johan't, Hart},
doi = {10.1121/1.385592},
issn = {NA},
journal = {Journal of the Acoustical Society of America},
month = {mar},
number = {3},
pages = {811--821},
publisher = {Acoustical Society of America},
title = {{Differential sensitivity to pitch distance, particularly in speech}},
url = {http://asa.scitation.org/doi/10.1121/1.385592},
volume = {69},
year = {1981}
}
@incollection{Kuhl2004,
abstract = {gives a historical perspective on the interactions that have occurred between psychoacoustics and speech perception research as well as a detailed account of her [the author's] recent work on the development of speech prototypes in human infants (PsycINFO Database Record (c) 2012 APA, all rights reserved). (preface)},
author = {Kuhl, Patricia K.},
booktitle = {Developmental psychoacoustics.},
doi = {10.1037/10119-012},
month = {oct},
pages = {293--332},
publisher = {American Psychological Association},
title = {{Psychoacoustics and speech perception: Internal standards, perceptual anchors, and prototypes.}},
year = {2004}
}
