Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Wildgruber2006,
abstract = {During acoustic communication in humans, information about a speaker's emotional state is predominantly conveyed by modulation of the tone of voice (emotional or affective prosody). Based on lesion data, a right hemisphere superiority for cerebral processing of emotional prosody has been assumed. However, the available clinical studies do not yet provide a coherent picture with respect to interhemispheric lateralization effects of prosody recognition and intrahemispheric localization of the respective brain regions. To further delineate the cerebral network engaged in the perception of emotional tone, a series of experiments was carried out based upon functional magnetic resonance imaging (fMRI). The findings obtained from these investigations allow for the separation of three successive processing stages during recognition of emotional prosody: (1) extraction of suprasegmental acoustic information predominantly subserved by right-sided primary and higher order acoustic regions; (2) representation of meaningful suprasegmental acoustic sequences within posterior aspects of the right superior temporal sulcus; (3) explicit evaluation of emotional prosody at the level of the bilateral inferior frontal cortex. Moreover, implicit processing of affective intonation seems to be bound to subcortical regions mediating automatic induction of specific emotional reactions such as activation of the amygdala in response to fearful stimuli. As concerns lower level processing of the underlying suprasegmental acoustic cues, linguistic and emotional prosody seem to share the same right hemisphere neural resources. Explicit judgment of linguistic aspects of speech prosody, however, appears to be linked to left-sided language areas whereas bilateral orbitofrontal cortex has been found involved in explicit evaluation of emotional prosody. These differences in hemispheric lateralization effects might explain that specific impairments in nonverbal emotional communication subsequent to focal brain lesions are relatively rare clinical observations as compared to the more frequent aphasic disorders. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Wildgruber, D. and Ackermann, H. and Kreifelts, B. and Ethofer, T.},
booktitle = {Progress in Brain Research},
doi = {10.1016/S0079-6123(06)56013-3},
issn = {00796123},
keywords = {affect,communication,emotion,fMRI,intonation,language,lateralization,prosody},
month = {jan},
pages = {249--268},
publisher = {Elsevier},
title = {{Chapter 13 Cerebral processing of linguistic and emotional prosody: fMRI studies}},
volume = {156},
year = {2006}
}
@article{Ohala1983,
abstract = {Certain signaling functions of the pitch of voice are remarkably similar across languages and cultures: (1) high or rising pitch to mark questions, low or falling pitch to mark nonquestions; (2) high pitch to signal politeness, low pitch to signal assertiveness; (3) in ‘sound symbolic' vocabulary, high tone used with words connoting smallness or diminutive, low tone with words connoting largeness. These patterns can be explained by the assumption that human vocal communication exploits the ‘frequency code', a cross-species association of high pitch vocalizations with smallness (of the vocalizer), lack of threat, and of low pitch vocalizations with the vocalizer's largeness and threatening intent. {\textcopyright} 1983 S. Karger AG, Basel.},
author = {Ohala, John J.},
doi = {10.1159/000261678},
issn = {1423-0321},
journal = {Phonetica},
keywords = {249722,No. 1,Phonetica 1983,Vol. 40},
number = {1},
pages = {1--18},
publisher = {Karger Publishers},
title = {{Cross-Language Use of Pitch: An Ethological View}},
url = {https://www.karger.com/Article/FullText/261678},
volume = {40},
year = {1983}
}
@article{Halbertsma2016,
abstract = {Emotions modulate cognitive processes, including those involved in the perception of time. A number of studies have demonstrated that the emotional modulation of interval timing can be described in terms of an attentional or an arousal-based mechanism, depending on the exact task setup. In this paper, two temporal generalization experiments with auditory emotional stimuli as distractors are presented. These experiments are modeled after the work by Lui et al. (PLoS One, 2011, 6, e218292011) who, using visual distractors, provided evidence for an attentional account of emotion-regulated modulation of the perception of time. Experiment 1 replicates the findings of Lui et al., and thus generalizes their work to auditory stimuli. However, Experiment 2, in setup highly similar to Experiment 1, failed to find any effects of emotional modulation on interval timing. These results indicate that emotional effects on interval timing, although often reported, might not be as ubiquitous as earlier research has (implicitly) suggested.},
author = {Halbertsma, Hinke N. and {Van Rijn}, Hedderik},
doi = {10.1163/22134468-00002061},
file = {:home/mhemmer/Downloads/MA-related/Literatur/acoustic cues/HalbertsmavanRijn2016AnEvaluationoftheEffectofAuditoryEmotionalStimulionIntervalTiming.pdf:pdf},
issn = {22134468},
journal = {Timing and Time Perception},
keywords = {Interval timing,attention vs. arousal,auditory stimuli,emotion,pacemaker-accumulator models,temporal modulation,time perception},
number = {1},
pages = {48--62},
title = {{An Evaluation of the Effect of Auditory Emotional Stimuli on Interval Timing}},
volume = {4},
year = {2016}
}
@article{Ohala1984,
abstract = {The author suggests that the following seemingly disparate phenomena have an underlying relationship: (a) cross-language similarities in the intonation contours for statements versus questions, (b) cross-cultural similarities in the vocal expression via intonation of attitude and affect, (c) cross-language patterns in the use of tone, vowels, and consonants in sound symbolic' vocabularly, (d) cross-species use of F0 in threatening or non threatening vocalizations, (e) cross-cultural and cross-species use of certain facial expressions (involving distinct mouth shape), and (f) the existence of sexual dimorphism in the vocal anatomy of humans (and certain non humans). He argues that all arise due to an innately specifiec 'frequency code', which associates high acoustic frequency with the primary meaning of 'small vocalizer' and thus such secondary meaning as 'subordinate, submissive, non threatening, desirous of the receiver's goodwill, etc.' and associates with low acoustic frequency the primary meaning of 'large vocalizer' and such secondary meanings as 'dominant, aggressive, threatening, etc.'},
author = {Ohala, John J.},
doi = {10.1159/000261706},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ohala1984.pdf:pdf},
issn = {00318388},
journal = {Phonetica},
keywords = {249717,No. 1,Phonetica 1984,Vol. 41},
number = {1},
pages = {1--16},
pmid = {6204347},
publisher = {Karger Publishers},
title = {{An ethological perspective on common cross-language utilization of F0 of voice}},
url = {https://www.karger.com/Article/FullText/261706},
volume = {41},
year = {1984}
}
@book{Wendt2007,
address = {Frankfurt am Main},
author = {Wendt, Beate},
isbn = {9783631563816},
publisher = {P. Lang},
title = {{Analysen emotionaler Prosodie}},
year = {2007}
}
@article{Lieberman1962,
abstract = {Pitch pulses were electronically derived from the utterances an objective statement, a fearful utterance, a happy utterance, etc. A fixed-vowel POVO-type synthesizer was excited by these pitch pulses. rate, could be smoothed original speech English who each read eight neutral test sentences The pitch perturbations, envelope Tapes were recorded in certain "emotional" modes, i.e., as a question, or rapid variations in the fundamental excitation out and the POVO could be amplitude-modulated amplitude. sented, correct identification was made 44{\%} of the time. When amplitude information was added to the pitch information, modes in forced judgment tests. Results of the tests show that with unpmcessed able to correctly stant reduced the identifications 120-cps monotone in 14{\%} identifications},
author = {Lieberman, Philip and Michaels, Sheldon B.},
doi = {10.1121/1.1918222},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jul},
number = {7},
pages = {922--927},
publisher = {Acoustical Society of America (ASA)},
title = {{Some Aspects of Fundamental Frequency and Envelope Amplitude as Related to the Emotional Content of Speech}},
url = {http://asa.scitation.org/doi/10.1121/1.1918222},
volume = {34},
year = {1962}
}
@article{VanLancker1973a,
abstract = {In past dichotic listening studies, linguistic stimuli have shown a right ear advantage, implying left hemisphere dominance for language processing, while other stimuli incorporating pitch distinctions have shown no ear perference or a left ear (right hemisphere) advantage. Ear preferences in tone language speakers were compared for 3 sets of stimuli: (a) pitch differences within language stimuli (tone-words in the tone language, Thai); (b) language stimuli without pitch differences (consonant-vowel words on mid-tone); and (c) pitch differences alone (hums). Results from 22 native Thai speakers demonstrate that tone-words and consonant-words are better heard at the right ear, while the hums show no ear effect. Preliminary results on 14 English-speaking Ss suggest that the consonant-words give the usual right ear effect, while the tone-words and the hums do not. It is concluded that pitch discrimination is lateralized to the left hemisphere when the pitch differences are linguistically processed. (47 ref.)},
author = {{Van Lancker}, Diana and Fromkin, Victoria A.},
doi = {10.1016/s0095-4470(19)31414-7},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Lancker, Fromkin - 1973 - Hemispheric specialization for pitch and “tone” Evidence from Thai.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
month = {apr},
number = {2},
pages = {101--109},
publisher = {Elsevier BV},
title = {{Hemispheric specialization for pitch and “tone”: Evidence from Thai}},
volume = {1},
year = {1973}
}
@article{Ackermann2004,
abstract = {Besides a sequence of words, spoken utterances are characterized by prosodic (suprasegmental) qualities such as a distinct into-nation contour ("speech melody"), loudness variations, and a rhythmic structure. In addition to a variety of linguistic and pragmatic functions, these features may reflect a speaker's mood and, thus, contribute, concomitant with facial and gestural movements, to the nonverbal expression of emotions (affective prosody). Clinical studies yielded discrepant data on the cerebral correlates of the processing of affective prosody. Functional imaging provides a more recent approach to the analysis of brain-behaviour relationships. The available investigations indicate two successive stages of the perceptual encoding of affective prosody: (a) predominant right-hemisphere processing of intonation contours within posterior parts of the superior temporal gyrus, (b) evaluation of the conveyed emotion at the level of bilateral orbitofrontal cortex. These findings corroborate and extend the model of a more proficient analysis and short-term storage of tonal information within the right cerebral hemisphere.},
author = {Ackermann, H. and Hertrich, I. and Crodd, W. and Wildgruber, D.},
doi = {10.1055/s-2004-828377},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/Ackermann{\_}Akt{\_}Neurol{\_}31{\_}446{\_}2004.pdf:pdf},
issn = {03024350},
journal = {Aktuelle Neurologie},
number = {9},
pages = {449--460},
title = {{"Das h{\"{o}}ren von gef{\"{u}}hlen": Funktionell-neuro-anatomische grundlagen der verarbeitung affektiver prosodie}},
volume = {31},
year = {2004}
}
@article{Droit-Volet2007,
abstract = {Our sense of time is altered by our emotions to such an extent that time seems to fly when we are having fun and drags when we are bored. Recent studies using standardized emotional material provide a unique opportunity for understanding the neurocognitive mechanisms that underlie the effects of emotion on timing and time perception in the milliseconds-to-hours range. We outline how these new findings can be explained within the framework of internal-clock models and describe how emotional arousal and valence interact to produce both increases and decreases in attentional time sharing and clock speed. The study of time and emotion is at a crossroads, and we outline possible examples for future directions. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Droit-Volet, Sylvie and Meck, Warren H.},
doi = {10.1016/j.tics.2007.09.008},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Droit-Volet, Meck - 2007 - How emotions colour our perception of time.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {dec},
number = {12},
pages = {504--513},
pmid = {18023604},
title = {{How emotions colour our perception of time}},
volume = {11},
year = {2007}
}
@inproceedings{Fenk-Oczlon2010,
abstract = {Basic language-inherent tempo cannot be isolated by the current metrics of speech rhythm. Here we propose the number of syllables per intonation unit as an appropriate measure, also for large-scale comparisons between languages. Applying it to an extended sample of in the meantime 51 languages has not only corroborated our previously reported negative cross-linguistic correlation of this metric with syllable complexity, but has revealed, moreover, significant correlations with several in part directly time-dependent rhythm measures proposed by other authors. We discuss relations between intrinsic tempo and (a) other facets of rhythm and (b) rhythm classifications of language. {\textcopyright} 2010 ISCA.},
author = {Fenk-Oczlon, Gertraud and Fenk, August},
booktitle = {Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
keywords = {Cross-linguistic correlations,Intonation unit,Metrics,Rhythm,Syllable complexity,Tempo},
pages = {1537--1540},
title = {{Measuring basic tempo across languages and some implications for speech rhythm}},
url = {https://www.researchgate.net/publication/221483780{\_}Measuring{\_}basic{\_}tempo{\_}across{\_}languages{\_}and{\_}some{\_}implications{\_}for{\_}speech{\_}rhythm},
year = {2010}
}
@article{Banse1996,
abstract = {Professional actors' portrayals of 14 emotions varying in intensity and valence were presented to judges. The results on decoding replicate earlier findings on the ability of judges to infer vocally expressed emotions with much-better-than-chance accuracy, including consistently found differences in the recognizability of different emotions. A total of 224 portrayals were subjected to digital acoustic analysis to obtain profiles of vocal parameters for different emotions. The data suggest that vocal parameters not only index the degree of intensity typical for different emotions but also differentiate valence or quality aspects. The data are also used to test theoretical predictions on vocal patterning based on the component process model of emotion (K. R. Scherer, 1986). Although most hypotheses are supported, some need to be revised on the basis of the empirical evidence. Discriminant analysis and jackknifing show remarkably high hit rates and patterns of confusion that closely mirror those found for listener-judges.},
annote = {* vocal emotional profiles for each emotion
* naturalness of acted emotions
* accuracy of prediction of emotions},
author = {Banse, Rainer and Scherer, Klaus R.},
doi = {10.1037/0022-3514.70.3.614},
file = {:home/mhemmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banse, Scherer - 1996 - Acoustic Profiles in Vocal Emotion Expression.pdf:pdf},
issn = {00223514},
journal = {Journal of Personality and Social Psychology},
number = {3},
pages = {614--636},
title = {{Acoustic Profiles in Vocal Emotion Expression}},
volume = {70},
year = {1996}
}
@article{Kraus2005,
abstract = {We have developed a data-driven conceptual framework that links two areas of science: the source-filter model of acoustics and cortical sensory processing streams. The source-filter model describes the mechanics behind speech production: the identity of the speaker is carried largely in the vocal cord source and the message is shaped by the ever-changing filters of the vocal tract. Sensory processing streams, popularly called 'what' and 'where' pathways, are well established in the visual system as a neural scheme for separately carrying different facets of visual objects, namely their identity and their position/motion, to the cortex. A similar functional organization has been postulated in the auditory system. Both speaker identity and the spoken message, which are simultaneously conveyed in the acoustic structure of speech, can be disentangled into discrete brainstem response components. We argue that these two response classes are early manifestations of auditory 'what' and 'where' streams in the cortex. This brainstem link forges a new understanding of the relationship between the acoustics of speech and cortical processing streams, unites two hitherto separate areas in science, and provides a model for future investigations of auditory function. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Kraus, Nina and Nicol, Trent},
doi = {10.1016/j.tins.2005.02.003},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {4},
pages = {176--181},
publisher = {Elsevier Ltd},
title = {{Brainstem origins for cortical 'what' and 'where' pathways in the auditory system}},
volume = {28},
year = {2005}
}
@article{Schirmer2004,
abstract = {Time is a fundamental dimension of behavior and as such underlies the perception and production of speech. This paper reviews patient and neuroimaging studies that investigated brain structures that support temporal aspects of speech. The left-frontal cortex, the basal ganglia, and the cerebellum represent structures that have been implicated repeatedly. A comparison with the structures involved in the timing of non-speech events (e.g., tones, lights, finger movements) suggests both commonalities and differences: while the basal ganglia and the cerebellum contribute to the timing of speech and non-speech events, the contribution of left-frontal cortex seems to be specific to speech or rapidly changing acoustic information. Motivated by these commonalities and differences, this paper presents assumptions about the function of basal ganglia, cerebellum, and cortex in the timing of speech. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Schirmer, Annett},
doi = {10.1016/j.cogbrainres.2004.04.003},
file = {:home/mhemmer/Downloads/MA-related/Literatur/emotion and prosody/schirmer2004.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Basal ganglia,CLT,Cerebellum,Cognition,Duration,Lateralization,Neural basis of behavior,Prosody,Tempo,VOT},
number = {2},
pages = {269--287},
title = {{Timing speech: A review of lesion and neuroimaging findings}},
volume = {21},
year = {2004}
}
